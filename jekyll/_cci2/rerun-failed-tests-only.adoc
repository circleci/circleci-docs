---
contentTags:
  platform:
  - Cloud
---
= Re-run failed tests only (preview)
:page-layout: classic-docs
:page-liquid:
:page-description: How to re-run only failed tests in a job and optimize credit usage.
:icons: font
:toc: macro
:toc-title:

WARNING: This feature is in **preview**, use at your own risk. This feature is not guaranteed to move to general availability. For questions and/or issues, please comment on our link:https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only/47775/[Community forum].

[#motivation-and-introduction]
== Motivation and introduction

CircleCI is releasing new preview functionality to **re-run failed tests only**. When selecting this option (see image below), only a subset of tests are re-run, instead of re-running the entire test suite when a transient test failure arises.

Historically, when your testing job in a workflow has flaky tests, the only option to get to a successful workflow was to link:https://support.circleci.com/hc/en-us/articles/360050303671-How-To-Rerun-a-Workflow[re-run your workflow from failed]. This type of re-run executes *all tests* from your testing job, including tests that passed, which prolongs time-to-feedback and consumes credits unnecessarily.

This re-run failed tests only option re-runs failed tests from the _same_ commit, not new ones.

image::{{site.baseurl}}/assets/img/docs/rerun-failed-tests-option.png[Option to rerun failed tests from Rerun menu]

[#prerequisites]
== Prerequisites

* Your testing job in the workflow is configured to xref:collect-test-data/#[upload test results] to CircleCI. `file` or `classname` attributes **must be present** in the xref:use-the-circleci-cli-to-split-tests#junit-xml-reports[JUnit XML output].
* The testing job uses `circleci tests run` (see below for details) to execute tests.
+
NOTE: If your current job is using xref:test-splitting-tutorial#[intelligent test splitting], you must *change* the `circleci tests split` command to `circleci tests run` combined with a `--split-by=` flag  (see below for instructions).

[#quickstart]
== Quickstart

[#example-config-file-before]
=== Before: Example `.circleci/config.yml` file

```yaml
 - run:
    name: Run tests
    command: |
      mkdir test-results
      TEST_FILES=$(circleci tests glob "**/test_*.py" | circleci tests split --split-by=timings) 
      pytest -o junit_family=legacy --junitxml=test-results/junit.xml $TEST_FILES
      
- store_test_results:
    path: test-results
```

This example snippet is from a CircleCI configuration file that:

. Executes Python test files that end in `.py`, 
. Splits tests by previous timing results (you can follow xref:test-splitting-tutorial#[this tutorial] on intelligent test splitting), 
. Stores the test results in a new directory called `test-results`, and 
. Uploads those test results to CircleCI.  

**Note:** `-o junit_family=legacy` is present to ensure that the test results being generated contain the `file` attribute. Not included in this snippet is the key to set parallelism (read the xref:parallelism-faster-jobs#[Test splitting and parallelism] page for more information).

[#example-config-file-after]
=== After: Example `.circleci/config.yml` file

In the snippet below, the example has been updated to use the `circleci tests run` command to allow for re-running only failed tests.

```yaml
 - run:
    name: Run tests
    command: |
      mkdir test-results
      TEST_FILES=$(circleci tests glob "**/test_*.py")
      echo "$TEST_FILES" | circleci tests run --command="xargs pytest -o junit_family=legacy --junitxml=test-results/junit.xml" --verbose --split-by=timings #--split-by=timings is optional, only use if you are using CircleCI's test splitting

 - store_test_results:
    path: test-results
```

* `TEST_FILES=$(circleci tests glob "**/test_*.py")`
+
Use CircleCI's xref:troubleshoot-test-splitting#video-troubleshooting-globbing[glob command] to put together a list of test files. In this case, we are looking for any test file that starts with `test_` and ends with `.py`. Ensure that the glob string is enclosed in quotes.
  
* `echo "$TEST_FILES" |`
+
Pass the list of test files to the `circleci tests run` command as standard input (link:https://www.computerhope.com/jargon/s/stdin.htm[`stdin`]).

* `circleci tests run --command="xargs pytest -o junit_family=legacy --junitxml=test-results/junit.xml" --verbose --split-by=timings`
  ** Invoke `circleci tests run` and specify the original command (`pytest`) used to run tests as part of the `--command=` parameter. **This is required**. `xargs` must be present as well.
  ** `--verbose` is an optional parameter for `circleci tests run` which enables more verbose debugging messages.
  ** *Optional*: `--split-by=timings` enables intelligent test splitting by timing for `circleci tests run`. Note that this is not required in order to use `circleci tests run`. If your testing job is not using CircleCI's test splitting, omit this parameter.
  
NOTE: You can specify the timing type similar to `circleci tests split --split-by=timings --timings-type=` using a `test-selector=` flag.  You can pass `file`, `classname`, or `name` (`name` splits by test name) to `--test-selector=`.
  
[#verify-the-configuration]
==== Verify the configuration

After updating your configuration, run the job that runs tests again and make sure that the same number of tests are being executed as before the config.yml change.  

Then, the next time you encounter a test failure on that job, click the "Re-run failed tests only" button.  If the `--verbose` setting is enabled, you should see output similar to the following the next time you click "Rerun failed tests only" with this job on CircleCI:

```sh
Installing circleci-tests-plugin-cli plugin.
circleci-tests-plugin-cli plugin Installed. Version: 1.0.5976-439c1fc
DEBU[2023-05-18T22:09:08Z] Attempting to read from stdin. This will hang if no input is provided.
INFO[2023-06-14T23:52:50Z] received failed tests from workflow ***** 
DEBU[2023-05-18T22:09:08Z] 2 test(s) failed out of 56 total test(s). Rerunning 1 test file(s)
DEBU[2023-06-14T23:52:50Z] if all tests are being run instead of only failed tests, ensure your JUnit XML has a file or classname attribute.
INFO[2023-05-18T22:09:08Z] starting execution                           
DEBU[2023-05-18T22:09:08Z] received test names: ****
```

If you see `rerunning failed tests` present in the step's output, the functionality is configured properly.  You'll also see output that shows the total number of failed tests from the original job run, the total number of 

The job should only re-run tests that are from a `classname` of `file` that had at least one test failure when the "Re-run failed tests only" button is clicked. If you are seeing different behavior, comment on this https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only/47775/[Discuss post] for support.

[#additional-examples]
== Additional examples

[#configure-a-job-running-ruby-rspec-tests]
=== Configure a job running Ruby (rspec) tests

. Add the following gem to your Gemfile:
+
```bash
gem 'rspec_junit_formatter'
```

. Modify your test command to use `circleci tests run`:
+
```yaml
 - run: mkdir ~/rspec
 - run:
    command: |
      circleci tests glob "spec/**/*_spec.rb" | circleci tests run --command="xargs bundle exec rspec --format progress --format RspecJunitFormatter -o ~/rspec/rspec.xml" --verbose --split-by=timings
```

. Update the `glob` command to match your use case. See the RSpec section in the xref:collect-test-data#rspec[Collect Test Data] document for details on how to output test results in an acceptable format for `rspec`. **If your current job is using xref:test-splitting-tutorial#[CircleCI's intelligent test splitting], you must change the `circleci tests split` command to `circleci tests run` with the `--split-by=timings` parameter.** If you are not using test splitting, `--split-by=timings` can be omitted.

[#configure-a-job-running-ruby-cucumber-tests]
=== Configure a job running Ruby (Cucumber) tests

. Modify your test command to look something similar to:
+
```yaml
- run: mkdir -p ~/cucumber
- run:
    command: |
    circleci tests glob "features/**/*.feature" | circleci tests run --command="xargs bundle exec cucumber --format junit --out ~/cucumber/junit.xml" --verbose --split-by=timings
```

. Update the `glob` command to match your use case. See the Cucumber section in the xref:collect-test-data#cucumber[Collect Test Data] document for details on how to output test results in an acceptable format for `Cucumber`. **If your current job is using xref:test-splitting-tutorial#[CircleCI's intelligent test splitting], you must change the `circleci tests split` command to `circleci tests run` with the `--split-by=timings` parameter.** If you are not using test splitting, `--split-by=timings` can be omitted.

[#configure-a-job-running-cypress-tests]
=== Configure a job running Cypress tests

. Use the link:https://www.npmjs.com/package/cypress-circleci-reporter[cypress-circleci-reporter] (note this is a 3rd party tool that is not maintained by CircleCI).  You can install in your `.circleci/config.yml` or add to your `package.json`. Example for adding to `.circleci/config.yml`:
+
```yaml
  #add required reporters (or add to package.json)
  -run:
    name: Install coverage reporter
    command: |
      npm install --save-dev cypress-circleci-reporter
```
+
. Use the `cypress-circleci-reporter`, `circleci tests run`, and upload test results to CircleCI:
+
```yaml
     -run:
        name: run tests
        command: | 
          mkdir test_results
          cd ./cypress 
          npm ci 
          npm run start &
          circleci tests glob "cypress/**/*.cy.js" | circleci tests run --command="xargs npx cypress run --reporter cypress-circleci-reporter --spec" --verbose --split-by=timings #--split-by=timings is optional, only use if you are using CircleCI's test splitting 
     
     - store_test_results
        path: test_results
```
+

Remember to modify the `glob` command for your specific use case.  **If your current job is using xref:test-splitting-tutorial#[CircleCI's intelligent test splitting], you must change the `circleci tests split` command to `circleci tests run` with the `--split-by=timings` parameter.** If you are not using test splitting, `--split-by=timings` can be omitted.

[#configure-a-job-running-javascript-typescript-jest-tests]
=== Configure a job running Javascript/Typescript (Jest) tests

. Install the `jest-junit` dependency. You can add this step in your `.circleci/config.yml`:
+
```yaml
  - run:
      name: Install JUnit coverage reporter
      command: yarn add --dev jest-junit
```
+
You can also add it to your `jest.config.js` file by following these link:https://www.npmjs.com/package/jest-junit[usage instructions].  

. Modify your test command to look something similar to:
+
```yaml
- run:
    command: |
      npx jest --listTests | circleci tests run --command=“JEST_JUNIT_ADD_FILE_ATTRIBUTE=true xargs npx jest --config jest.config.js --runInBand --” --verbose --split-by=timings
    environment:
      JEST_JUNIT_OUTPUT_DIR: ./reports/     
  - store_test_results:
      path: ./reports/
```

. Update the `npx jest --listTests` command to match your use case. See the Jest section in the xref:collect-test-data#jest[Collect Test Data] document for details on how to output test results in an acceptable format for `jest`. **If your current job is using xref:test-splitting-tutorial#[CircleCI's intelligent test splitting], you must change the `circleci tests split` command to `circleci tests run` with the `--split-by=timings` parameter.** If you are not using test splitting, `--split-by=timings` can be omitted.
+
`JEST_JUNIT_ADD_FILE_ATTRIBUTE=true` is added to ensure that the `file` attribute is present. `JEST_JUNIT_ADD_FILE_ATTRIBUTE=true` can also be added to your `jest.config.js` file instead of including it in `.circleci/config.yml`, by using the following attribute: `addFileAttribute="true"`.

[#configure-a-job-running-playwright-tests]
=== Configure a job running Playwright tests

. Modify your test command to use `circleci tests run`:
+
```yaml
 - run:
    command: |
      mkdir test-results #can also be switched out for passing PLAYWRIGHT_JUNIT_OUTPUT_NAME directly to Playwright
      pnpm run serve &
      TESTFILES = $(circleci tests glob "specs/e2e/**/*.spec.ts")
      echo "$TESTFILES" | circleci tests run --command="xargs pnpm playwright test --config=playwright.config.ci.ts --reporter=junit" --verbose --split-by=timings
```

. Update the `glob` command to match your use case. **If your current job is using xref:test-splitting-tutorial#[CircleCI's intelligent test splitting], you must change the `circleci tests split` command to `circleci tests run` with the `--split-by=timings` parameter.**. If you are not using test splitting, `--split-by=timings` can be omitted. Note: you may also use link:https://playwright.dev/docs/test-reporters#junit-reporter[Playwright's built-in flag] (`PLAYWRIGHT_JUNIT_OUTPUT_NAME`) to specify the JUnit XML output directory.  
+
NOTE: Ensure that you are using version 1.34.2 or later of Playwright. Earlier versions of Playwright may not output JUnit XML in a format that is compatible with this feature.

[#output-test-files-only]
=== Output test files only

If your testing set-up on CircleCI is not compatible with invoking your test runner in the `circleci tests run` command, you can opt to use `circleci tests run` to receive the file names, output the file names, and save the file names to a temporary location.  You can then subsequently invoke your test runner using the outputted file names.  

Example:

```yaml
 - run:
    command: |
      circleci tests glob "src/**/*js" | circleci tests run --command ">files.txt xargs echo" --verbose --split-by=timings > files.txt #split-by=timings is optional
```

The snippet above will write the list of test file names to `files.txt`.  On a non-rerun, this list will be all of the test file names.  On a "re-run", the list will be a subset of file names (the test file names that had at least 1 test failure in the previous run).  You can pass the list of test file names from `files.txt` into, for example, your custom `makefile`.

[#known-limitations]
== Known limitations

* When re-running only the failed tests, test splitting by timing may not be as efficient as expected the next time that job runs, as the test results being stored are only from the subset of failed tests that were run.
* Orbs that run tests may not work with this new functionality at this time.
* If a shell script is invoked to run tests, `circleci tests run` should be placed **in the shell script** itself, and not `.circleci/config.yml`.
* Jobs that are older than the xref:persist-data#custom-storage-usage[retention period] for workspaces for the organization cannot be re-run with "Re-run failed tests only".
* Jobs that upload code coverage reports link:https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only-circleci-tests-run/47775/3?u=sebastian-lerner[may see issues during a re-run].

[#troubleshooting]
== Troubleshooting

After configuring `circleci tests run`, if you see *all tests* are re-run after clicking "Rerun failed tests only":

1. Ensure that the `--verbose` setting is enabled when invoking `circleci tests run`. This will display what "tests" `circleci tests run` is receiving on a "re-run".
2. Use xref:configuration-reference#storeartifacts[`store_artifacts`] to upload  the JUnit XML that contains test results to CircleCI.  This is the same file(s) that is being uploaded to CircleCI with `store_test_results`
3. Manually inspect the newly uplaoded JUnit XML via the **Artifacts** tab and ensure that there is a `file=` attribute or a `classname` attribute.  If neither are present, you will see unexpected behavior when trying to re-run.  Follow the instructions on this page to ensure that the test runner you are using is outputting its JUnit XML test results with a `file` (preferred) or `classname` attribute.  Comment in our link:https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only-circleci-tests-run/47775/48[community forum] if you are still stuck.

[#FAQs]
== FAQs

**Question:** I have a question or issue, where do I go?

**Answer:** Leave a comment on the https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only/47775/[Discuss post].

---

**Question:** Will this functionality re-run individual tests?

**Answer:** No, it will re-run failed test `classnames` or `file` that had at least one individual test failure.

---

**Question:** What happens if I try to use the functionality but `circleci tests run` is not used in the `.circleci/config.yml` file?

**Answer:** All tests will be executed when the workflow runs again, including failed tests. This is equivalent to selecting "Rerun workflow from failed".  

---

**Question:** What happens if I try to use the functionality and `circleci tests run` is used in my `.circleci/config.yml` file, but I have not configured my job to upload test results to CircleCI?

**Answer:** The job will fail.

---

**Question:** When can I click the option to "Re-run failed tests only?"

**Answer:** Currently, it will be present any time the "Re-run workflow from failed" option is present, and vice versa.

---

**Question:** I don't see my test framework on this page, can I still use the functionality?

**Answer:** Yes, as long as your job meets the xref:#prerequisites[prerequisites] outlined above. The re-run failed tests only functionality is test runner- and test framework-agnostic. You can use the methods described in the xref:collect-test-data#[Collect test data] document to ensure that the job is uploading test results. Note that `classname` and `file` is not always present by default, so your job may require additional configuration.  

From there, follow the xref:#quickstart[Quickstart] section to modify your test command to use `circleci tests run`. 

If you run into issues, comment on the https://discuss.circleci.com/t/product-launch-re-run-failed-tests-only/47775/[Discuss post].

---

**Question:** Can I see in the web UI whether a job was re-run using "Re-run failed tests only"?

**Answer:** Not at this time.

---

**Question:** What happens when a job is re-run and it is using parallelism and test splitting?

**Answer:** The job will spin up the number of containers/virtual machines (VMs) that are specified with the `parallelism` key.  However, the step that runs tests for each of those parallel containers/VMs will only run a subset of tests, or no tests, after the tests are split across the total number of parallel containers/VMs.  For example, if `parallelism` is set to eight, there may only be enough tests after the test splitting occurs to "fill" the first two parallel containers/VMs. The remaining six containers/VMs will still start up, but they will not run any tests when they get to the test execution step.

---

**Question:** My maven surefire tests are failing when I try to set this feature up?

**Answer:** You may need to add the `-DfailIfNoTests=false` flag to ensure the testing framework ignores skipped tests instead of reporting a failure when it sees a skipped test on a dependent module. 

---

**Question:** Can I specify timing type for test splitting using `circleci tests run`?

**Answer:** Yes, you can specify the timing type similar to `circleci tests split --split-by=timings --timings-type=` using a `test-selector` flag.  You can pass `file`, `classname`, or `name` (`name` splits by test name).

---

**Question:** Why does the re-run appear to be running all tests instead of only failed tests?

**Answer:** The most common case where this occurs is when the JUnit XML is not outputting a "file" attribute.  If you upload your test results XML to an artifact, you can inspect whether or not it has the "file" attribute.  

---

**Question:** Are tests that were reported by my test runner as "Skipped" or "Ignored" re-run when I click "Rerun failed tests only"?

**Answer:** No, only test files that have at least one test case reported as "Failed" will be re-run.  
---

