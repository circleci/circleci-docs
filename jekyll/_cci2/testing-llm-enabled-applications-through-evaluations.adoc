---
contentTags:
  platform:
    - Cloud
    - Server
---
= Testing LLM-enabled applications through evaluations
:description: Docs page on setting up LLM evaluations in your CI/CD pipeline
:page-layout: classic-docs
:page-liquid:
:icons: font
:experimental:

This page describes common methods for testing applications powered by large language models (LLMs) through evaluations.

== Evaluations overview

Evaluations, also known as evals, are a methodology for assessing the quality of AI software.

Evaluations provide insights into the performance and efficiency of applications based on LLMs, and allow teams to quantify how well their AI implementation works, measure improvements, and catch regressions.

The term _evaluation_ initially referred to a way to rate and compare AI models. It has since expanded to include application-level testing, including Retrieval Augmented Generation (RAG), function calling, and agent-based applications.

The evaluation process involves using a dataset of inputs to an LLM or application code, and a method to determine if the returned response matches the expected response. There are many evaluation methodologies, such as:

* LLM-assisted evaluations
* Human evaluations
* Intrinsic and extrinsic evaluations
* Bias and fairness checks
* Readability evaluations

Evaluations can cover many aspects of a model performance, including:

* Ability to comprehend specific jargon
* Make accurate predictions
* Avoid hallucinations and generate relevant content
* Respond in a fair and unbiased way, and within a specific style
* Avoid certain expressions

Using an open source library or third-party tools can simplify defining evaluations, tracking progress, and reviewing evaluation results.

== Automating evaluations with CircleCI

*Evaluations can be automated in CircleCI pipelines.* Automating evaluations with CircleCI can enable you to automatically determine whether or not to accept proposed AI application changes, or deploy and release your AI application.
There are important differences between evals and standard software tests to keep in mind:


* LLMs are predominantly non-deterministic, leading to flaky evaluations, more so than typical deterministic software tests.
* Evaluation results are subjective. Small regressions in a metric might not necessarily be a cause for concern, unlike failing tests in standard software testing.
* cause for concern, unlike failing tests in standard software testing.
Evaluation results are manually reviewed to determine whether AI application performance meets expectations since the output metric values can fluctuate.

With CircleCI, you can define, automate, run evaluations and evaluation testing using your preferred evaluation framework. Through declaring the necessary commands in your `config.yml`,  you can ensure evaluations are run and evaluation results are tested within your CircleCI pipeline. 

You can automate your entire process of running evaluations, reviewing evaluation results and determining whether results meet expectations, removing manual work involved with triggering evaluations and reviewing evaluation results.

== The CircleCI Evals orb

[NOTE]
====
The link:https://circleci.com/developer/orbs/orb/circleci/evals[official Evals orb] supports integrations with Braintrust and LangSmith.

If you run evaluations using a different tool, let us know at mailto:ai-feedback@circleci.com[]. You can also contribute directly to the official orb by opening a PR on the link:https://github.com/CircleCI-Public/ai-evals-orb[public repository].
====

CircleCI provides an link:https://circleci.com/developer/orbs/orb/circleci/evals[official Evals orb] which:
* Simplifies the definition and execution of evaluation jobs using custom scripts and popular third-party tools
* Generates reports of evaluation results
* Tests evaluation results to determine if they meet specified conditions

== Running Evaluations in CircleCI

With the CircleCI Evals orb, evaluations:
* Will run as part of your CircleCI pipeline
* Job step details can include direct links to your evaluation results stored on your 3rd party evaluations provider. When you need to further review the evaluation results in detail, this link enables you to easily navigate to and view your evaluation results.
* Your evaluation results can be saved to a `.json` file, for evaluation testing.

image::/docs/assets/img/docs/llmops/eval-job-run-eval-step.png[Evaluations Job Step Details: Task Evals]

== Evaluation Testing

Evaluation Testing enables you to determine if your evaluation results meet specified conditions. CircleCI Evaluation Testing is a suite of test cases. Each test case represents a scenario with an assertion to check if a specified condition is true or false. If any test case fails, the evaluation test fails.

You can configure test case assertions such as

. Thresholds: Check whether 1 or more results fall within a range
+
[,shell]
----
{                                                    
  “TestCaseName”: “correctness > 0.99 && toxicity < 0.01”
}
----

. Equality: Check whether results provide an expected answer
+
[,shell]
----
{                                                    
  “TestCaseName”: “label == \”CORRECT\””
}
----

Evaluation Testing results determine if a workflow should stop or continue:
* **If the evaluation test fails**: Indicates a proposed change resulted in a degradation of model performance. The workflow will stop running.
* **If the evaluation test passes**: Indicates that model performance has met set criteria, the workflow will continue running.

== View Evaluation Testing Results

Evaluation Testing determines if your evaluation results meet specified conditions. Evaluation Testing results are presented in the CircleCI web UI in two locations:

. In the step details
+
image::/docs/assets/img/docs/llmops/eval-job-eval-test-step.png[Evaluation Testing: Job Step Details]

. In the tests tab. Additionally, when a test case has failed, its details are displayed.
+
image::/docs/assets/img/docs/llmops/eval-test-fail-detail.png[Evaluation Testing: Test Failure Details]

== Storing credentials for your evaluations
CircleCI makes it easy to store your credentials for LLM providers as well as LLMOps tools. 

* Navigate to **Project Settings > LLMOps** to enter, verify, and access your OpenAI secrets. 
* Here, you can also find a starting template for your `config.yml` file.
* You can save the credentials for your evaluation platform, including Braintrust and LangSmith. These credentials can then be used when setting up a pipeline that uses the Evals orb.

image::/docs/assets/img/docs/llmops/create-context.png[Project Settings > LLMOPS: Create Context Modal Window in CircleCI]
image::/docs/assets/img/docs/llmops/openai-context.png[Project Settings > LLMOPS: View contexts in CircleCI]


