---
contentTags:
  platform:
    - Cloud
    - Server
---
= Testing LLM-enabled applications through evaluations
:description: Docs page on setting up LLM evaluations in your CI/CD pipeline
:page-layout: classic-docs
:page-liquid:
:icons: font
:experimental:

This page describes common methods for testing applications powered by large language models (LLMs) through evaluations.

== Evaluations overview

Evaluations, also known as evals, are a methodology for assessing the quality of AI software.

Evaluations provide insights into the performance and efficiency of applications based on LLMs, and allow teams to quantify how well their AI implementation works, measure improvements, and catch regressions.

The term _evaluation_ initially referred to a way to rate and compare AI models. It has since expanded to include application-level testing, including Retrieval Augmented Generation (RAG), function calling, and agent-based applications.

The evaluation process involves using a dataset of inputs to an LLM or application code, and a method to determine if the returned response matches the expected response. There are many evaluation methodologies, such as:

* LLM-assisted evaluations
* Human evaluations
* Intrinsic and extrinsic evaluations
* Bias and fairness checks
* Readability evaluations

Evaluations can cover many aspects of a model performance, including:

* Ability to comprehend specific jargon
* Make accurate predictions
* Avoid hallucinations and generate relevant content
* Respond in a fair and unbiased way, and within a specific style
* Avoid certain expressions

Using an open source library or third-party tools can simplify defining evaluations, tracking progress, and reviewing evaluation results.

== Automating evaluations with CircleCI

*Evaluations can be automated in CircleCI pipelines.* Automating evaluations with CircleCI can enable you to automatically determine whether or not to accept proposed AI application changes, or deploy and release your AI application.
There are important differences between evals and standard software tests to keep in mind:

* LLMs are predominantly non-deterministic, leading to flaky evaluations, more so than typical deterministic software tests.
* Evaluation results are subjective. Small regressions in a metric might not necessarily be a cause for concern, unlike failing tests in standard software testing.
* Evaluation results are manually reviewed to determine whether AI application performance meets expectations since the output metric values can fluctuate.

With CircleCI, you can define, automate, run evaluations and evaluation testing using your preferred evaluation framework. Through declaring the necessary commands in your `config.yml`,  you can ensure evaluations are run and evaluation results are tested within your CircleCI pipeline.

You can automate your entire process of running evaluations, reviewing evaluation results and determining whether results meet expectations, removing manual work involved with triggering evaluations and reviewing evaluation results.

== The CircleCI Evals orb

[NOTE]
====
The link:https://circleci.com/developer/orbs/orb/circleci/evals[official Evals orb] supports integrations with Braintrust and LangSmith.

If you run evaluations using a different tool, let us know at mailto:ai-feedback@circleci.com[]. You can also contribute directly to the official orb by opening a PR on the link:https://github.com/CircleCI-Public/ai-evals-orb[public repository].
====

CircleCI provides an link:https://circleci.com/developer/orbs/orb/circleci/evals[official evals orb], which offers the following capabilities:

* Simplifies the definition and execution of evaluation jobs using custom scripts and popular third-party tools.
* Generates reports of evaluation results.
* Tests evaluation results to determine if they meet specified conditions.

=== Running Evaluations in CircleCI

With the CircleCI evals orb, evaluations:

* Will run as part of your CircleCI pipeline.
* Job step details can include direct links to your evaluation results stored on your 3rd party evaluations provider. When you need to further review the evaluation results in detail, this link enables you to easily navigate to and view your evaluation results.
* Your evaluation results can be saved to a `.json` file, for evaluation testing.

image::/docs/assets/img/docs/llmops/eval-job-run-eval-step.png[Evaluations Job Step Details: Task Evals]

=== Evaluation testing

Evaluation testing enables you to determine if your evaluation results meet specified conditions. CircleCI Evaluation Testing is a suite of test cases. Each test case represents a scenario with an assertion to check if a specified condition is true or false. If any test case fails, the evaluation test fails.

++++
<div style="background-color: #F7F7F7; margin: 2rem auto; max-width: 860px; padding: 2rem; border-radius: 0.2rem;">
  <h2 style="margin-top: 0; text-align: center;">What is evaluation testing?</h2>
  <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 400px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">LLM Evaluations</h4><p style="margin-bottom: 0;">Methods used to assess the performance and accuracy of AI models and algorithms. Evals measure metrics such as task fidelity, consistency, relevance and coherence, or tone and style.</p>
    </div>
  
    <div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
     <div style="border-right: 1px solid #404040;"></div> 
        <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">LLM evaluations output</p>
    </div>
  
      <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 400px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Evaluation results</h4><p style="margin-bottom: 0;">The set of quantitative (often numeric) results that are output by running an Evaluation.</p>
    </div>


<div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
  <div style="border-left: 1px solid #404040;">
    
    </div> 
    <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">Evaluation results are provided as</p>
</div>
  


          <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 400px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Metrics</h4><p style="margin-bottom: 0;">Refers to 1 or more units of data, used as inputs for testing.</p>
    </div>

<div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
  <div style="border-left: 1px solid #404040;"></div>
    <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">Metrics are the inputs for</p>
</div>

         <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 400px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Evaluation testing</h4><p style="margin-bottom: 0;">A test that determines if input metrics meet specified conditions.</p>
    </div>
    

<div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
  <div style="border-left: 1px solid #404040;"></div>
  <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">Evaluation testing is composed of a</p>
</div>
    
             <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 400px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Test suite</h4><p style="margin-bottom: 0;">A set of test cases that are related and executed together. Is a <span style="background-color: #f1f1f1; font-family: monospace;">.json</span> file that is assigned a name by the user.</p>
    </div>

<div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
  <div style="border-left: 1px solid #404040;"></div>
  <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">A test suite is made up of one or more</p>
</div>
    
    
                 <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 648px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
                  
    <h4 style="margin:0;">Test case</h4><p>A test scenario, tests a single concept to make it clear what has failed when the test does not pass. Each has a:</p>
                    <div id="row" style="display: flex; flex-direction: row; gap: 8px;">
                   <div id="card-inner" style="border-radius: 8px; border: 1px solid #BFBFBF; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Name</h4><p style="margin-bottom: 0;">The name assigned to the test case. Usually descriptive of the scenario.</p>
    </div>
                                      <div id="card-inner" style="border-radius: 8px; border: 1px solid #BFBFBF; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Assertion</h4><p style="margin-bottom: 0;">A statement that checks if the specified condition is true. If false, the test fails.</p>
    </div>
                                      <div id="card-inner" style="border-radius: 8px; border: 1px solid #BFBFBF; padding: 1rem; margin: 0 auto; background-color: #FFF;">
    <h4 style="margin:0;">Result</h4><p style="margin-bottom: 0;">Each test case has a result of pass or fail.</p>
    </div>
                   </div>
    </div>


<div id="connector" style="margin: 0 auto;display: flex;flex-direction:row;gap: 16px;max-width: 400px;padding-left: 4rem;"> 
  <div style="border-left: 1px solid #404040;"></div>
  <p style=" margin-bottom: 0; margin: 0.5rem; font-size: 0.8rem;">Once all test cases are evaluated, returns</p>
</div>
    
     <div id="card" style="border-radius: 8px; border: 1px solid #BFBFBF; max-width: 648px; padding: 1rem; margin: 0 auto; background-color: #FFF;">
                  
    <h4 style="margin:0 0 0.5rem 0;">Evaluation test results</h4>
                    <div id="row" style="display: flex; flex-direction: row; gap: 8px;">

                                      <div id="card-inner" style="border-radius: 8px; border: 1px solid #BFBFBF; padding: 1rem; margin: 0 auto; background-color: #FFF; width: 50%;">
                                        <svg width="36" height="26" viewBox="0 0 46 36" fill="none" xmlns="http://www.w3.org/2000/svg">
                                          <path d="M0.5 18C0.5 8.05888 8.55888 0 18.5 0H27.5C37.4411 0 45.5 8.05888 45.5 18C45.5 27.9411 37.4411 36 27.5 36H18.5C8.55887 36 0.5 27.9411 0.5 18Z" fill="#94E5AB"></path>
                                          <g clip-path="url(#clip0_87_1823)">
                                          <path fill-rule="evenodd" clip-rule="evenodd" d="M33.1429 10.4278C33.597 10.8519 33.6213 11.5638 33.1972 12.0179L20.5894 25.5179C20.3782 25.744 20.0831 25.8732 19.7736 25.875C19.4642 25.8767 19.1676 25.7509 18.9538 25.5272L12.8116 19.0986C12.3824 18.6494 12.3986 17.9373 12.8478 17.508C13.2971 17.0788 14.0092 17.095 14.4384 17.5443L19.7579 23.1117L31.5528 10.4821C31.9769 10.0281 32.6888 10.0037 33.1429 10.4278Z" fill="#00381A"></path>
                                          </g>
                                          <defs>
                                          <clipPath id="clip0_87_1823">
                                          <rect width="21" height="21" fill="white" transform="translate(12.5 7.5)"></rect>
                                          </clipPath>
                                          </defs>
                                          </svg>
    <h4 style="margin:0;">Success</h4><p style="margin-bottom: 0;">Indicates that model performance has met set criteria, the job will continue running.</p>
    </div>

    <div id="card-inner" style="border-radius: 8px; border: 1px solid #BFBFBF; padding: 1rem; margin: 0 auto; background-color: #FFF; width: 50%;">

      <svg width="36" height="26" viewBox="0 0 46 36" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M0 18C0 8.05888 8.05888 0 18 0H27C36.9411 0 45 8.05888 45 18C45 27.9411 36.9411 36 27 36H18C8.05887 36 0 27.9411 0 18Z" fill="#CC4242"></path>
        <path fill-rule="evenodd" clip-rule="evenodd" d="M13.8295 9.3295C14.2688 8.89017 14.9812 8.89017 15.4205 9.3295L22.5 16.409L29.5795 9.3295C30.0188 8.89017 30.7312 8.89017 31.1705 9.3295C31.6098 9.76884 31.6098 10.4812 31.1705 10.9205L24.091 18L31.1705 25.0795C31.6098 25.5188 31.6098 26.2312 31.1705 26.6705C30.7312 27.1098 30.0188 27.1098 29.5795 26.6705L22.5 19.591L15.4205 26.6705C14.9812 27.1098 14.2688 27.1098 13.8295 26.6705C13.3902 26.2312 13.3902 25.5188 13.8295 25.0795L20.909 18L13.8295 10.9205C13.3902 10.4812 13.3902 9.76884 13.8295 9.3295Z" fill="white"></path>
        </svg>
        
      <h4 style="margin:0;">Failure</h4><p style="margin-bottom: 0;">Indicates a proposed change led to a degradation of model performance. The job will stop running, and the pipeline will fail.</p>
      </div>
                                    
                   </div>
    </div>
  

  
</div>
++++


You can configure test case assertions such as:

* Thresholds: Check whether 1 or more results fall within a range.
+
[,shell]
----
{
  “TestCaseName”: “correctness > 0.99 && toxicity < 0.01”
}
----

* Equality: Check whether results provide an expected answer.
+
[,shell]
----
{
  “TestCaseName”: “label == \”CORRECT\””
}
----

Evaluation testing results determine if a job should stop or continue:

* **If the evaluation test fails**: Indicates a proposed change resulted in a degradation of model performance. The job will stop running, and the pipeline will fail.
* **If the evaluation test passes**: Indicates that model performance has met set criteria, the job will continue running.

=== View Evaluation testing results

Evaluation testing determines if your evaluation results meet specified conditions. Evaluation testing results are presented in the CircleCI web UI in two locations:

* In the step details
+
image::/docs/assets/img/docs/llmops/eval-job-eval-test-step.png[Evaluation testing: Job Step Details]

* In the tests tab. Additionally, when a test case has failed, its details are displayed.
+
image::/docs/assets/img/docs/llmops/eval-test-fail-detail.png[Evaluation testing: Test Failure Details]

== Storing credentials for your evaluations
CircleCI makes it easy to store your credentials for LLM providers as well as LLMOps tools.

* Navigate to **Project Settings > LLMOps** to enter, verify, and access your OpenAI secrets.
* Here, you can also find a starting template for your `config.yml` file.
* You can save the credentials for your evaluation platform, including Braintrust and LangSmith. These credentials can then be used when setting up a pipeline that uses the Evals orb.
* To get started, navigate to **Project Settings > LLMOps**

image::/docs/assets/img/docs/llmops/create-context.png[Project Settings > LLMOPS: Create Context Modal Window in CircleCI]
image::/docs/assets/img/docs/llmops/openai-context.png[Project Settings > LLMOPS: View contexts in CircleCI]


