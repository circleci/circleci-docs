---
version:
- Server v4.x
- Server Admin
---
= Phase 3 - Execution environments
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

// This doc uses ifdef and ifndef directives to display or hide content specific to Google Cloud Storage (env-gcp) and AWS (env-aws). Currently, this affects only the generated PDFs. To ensure compatability with the Jekyll version, the directives test for logical opposites. For example, if the attribute is NOT env-aws, display this content. For more information, see https://docs.asciidoctor.org/asciidoc/latest/directives/ifdef-ifndef/.

Before you begin with the CircleCI server v4.x execution installation phase, ensure you have run through <<phase-1-prerequisites#,Phase 1 – Prerequisites>> and <<phase-2-core-services#,Phase 2 - Core Services Installation>>.

.Installation Experience Flow Chart Phase 3
image::server-install-flow-chart-phase3.png[Flow chart showing the installation flow for server 3.x with phase 3 highlighted]

NOTE: In the following sections, replace any items or credentials displayed between `< >` with your details.

toc::[]

== Nomad Clients
As mentioned in the link:https://circleci.com/docs/2.0/server-3-overview[Overview], Nomad is a workload orchestration tool that CircleCI uses to schedule (through Nomad Server) and run (through Nomad Clients) CircleCI jobs.

Nomad clients are installed outside of the Kubernetes cluster, while their control plane (Nomad Server) is installed within the cluster. Communication between your Nomad Clients and the Nomad control plane is secured with mTLS. The mTLS certificate, private key, and certificate authority will be output after you complete installation of the Nomad Clients.

=== Cluster Creation with Terraform

CircleCI curates Terraform modules to help install Nomad clients in your chosen cloud provider. You can browse the modules in our link:https://github.com/CircleCI-Public/server-terraform[public repository], including example Terraform config files for both AWS and GCP.

// Don't include this section in the GCP PDF:
ifndef::env-gcp[]

==== AWS
You need some information about your cluster and server installation to populate the required variables for the Terraform module. A full example, as well as a full list of variables, can be found link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[here].

* *Server_endpoint* - This is the domian name of the CircleCI application appended with port number `4647`.

* *Subnet ID (subnet)*, *VPC ID (vpc_id)*, and *DNS server (dns_server)* of your cluster.
Run the following command to get the cluster VPC ID (vpcId), and subnets (subnetIds):
+
```shell
aws eks describe-cluster --name=<YOUR_CLUSTER_NAME>
```
+
This returns something similar to the following:
+
[source, json]
{...
"resourcesVpcConfig": {
    "subnetIds": [
        "subnet-033a9fb4be69",
        "subnet-04e89f9eef89",
        "subnet-02907d9f35dd",
        "subnet-0fbc63006c5f",
        "subnet-0d683b6f6ba8",
        "subnet-079d0ca04301"
    ],
    "clusterSecurityGroupId": "sg-022c1b544e574",
    "vpcId": "vpc-02fdfff4c",
    "endpointPublicAccess": true,
    "endpointPrivateAccess": false
...
"kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16"
        },
...
}
+
Then, using the VPCID you just found, run the following command to get the CIDR Block for your cluster. For AWS, the DNS Server is the third IP in your CIDR block (`CidrBlock`), for example your CIDR block might be `10.100.0.0/16`, so the third IP would be `10.100.0.2`.
+
```shell
aws ec2 describe-vpcs --filters Name=vpc-id,Values=<YOUR_VPCID>
```
+
This returns something like the following:
+
[source, json]
{...
"CidrBlock": "192.168.0.0/16",
"DhcpOptionsId": "dopt-9cff",
"State": "available",
"VpcId": "vpc-02fdfff4c"
...}


Once you have filled in the appropriate information, you can deploy your Nomad clients by running the following command:

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it outputs the certificates and keys needed for configuring the Nomad control plane in CircleCI server. Copy them somewhere safe. The apply process usually only takes a minute.

// Stop hiding from GCP PDF:

endif::env-gcp[]
// Don't include this section in the AWS PDF:

ifndef::env-aws[]

==== GCP

You need the following information:

* The Domain name of the CircleCI Application
* The GCP Project you want to run Nomad clients in.
* The GCP Zone you want to run Nomad clients in.
* The GCP Region you want to run Nomad clients in.
* The GCP Network you want to run Nomad clients in.
* The GCP Subnetwork you want to run Nomad clients in.

A full example, as well as a full list of variables, can be found link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[here].

Once you have filled in the appropriate information, you can deploy your Nomad clients by running the following commands:

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it outputs the certificates and key needed for configuring the Nomad control plane in CircleCI server. Copy them somewhere safe.

endif::env-aws[]

=== Nomad Autoscaler
Nomad can automatically scale up or down your Nomad clients, provided your clients are managed by a cloud provider's autoscaling resource. With Nomad Autoscaler, you need to provide permission for the utility to manage your autoscaling resource and specify where it is located. CircleCI's Nomad terraform module can provision the permissions resources, or it can be done manually.

ifndef::env-gcp[]

[[nomad-autoscaler-aws]]
==== AWS
Create an IAM user or role and policy for Nomad Autoscaler. You may take one of the following approaches:

. Our link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[nomad module] creates an IAM user and outputs the keys if you set variable `nomad_auto_scaler = true`. You may reference the example in the link for more details. If you have already created the clients, you can update the variable and run `terraform apply`. The created user's access key and secret will be available in Terraform's output.
. You may also create a Nomad Autoscaler IAM user manually with the IAM policy below. Then you need to generate an access and secret key for this user.
. You may create a https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[Role for Service Accounts] for Nomad Autoscaler and attach the IAM policy below:

*IAM policy for Nomad Autoscaler*
[source, json]
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:CreateOrUpdateTags",
                "autoscaling:UpdateAutoScalingGroup",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "<<Your Autoscaling Group ARN>>"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeScalingActivities",
                "autoscaling:DescribeAutoScalingGroups"
            ],
            "Resource": "*"
        }
    ]
}

endif::env-gcp[]

ifndef::env-aws[]

[[nomad-autoscaler-gcp]]
==== GCP
Create a service account for Nomad Autoscaler.

. Our link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[nomad module] can create a service account and outputs a file with the keys.  Set the variable `nomad_auto_scaler = true`. You may reference the examples in the link for more details.  The created user's key will be available in a file named `nomad-as-key.json`.
. Our link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[nomad module] can create a service account using link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload Identities]  and outputs the email.  Set the variables `nomad_auto_scaler = true` and `enable_workload_identity = true`.
. You may also create a nomad gcp service account manually. The service account will need the role `compute.admin`. It will also need the role `iam.workloadIdentityUser` if using link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload identities]

Enable workload identity for `nomad-autoscaler` (kubernetes) service account
```shell
gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/nomad-autoscaler]"
```

endif::env-aws[]

== Nomad Servers

Now that you have successfully deployed your Nomad clients and have the permission resources, you can configure the Nomad servers.

. In your helm values.yaml file add a `nomad` section.  The `CACertificate`, `certificate` and `privateKey` can be found in the output of the terraform module.  They must be base64 encoded.
```
nomad:
  server:
    gossip:
      encryption:
        enabled: true
    rpc:
      mTLS:
        enabled: true
        CACertificate: <base64-encoded-ca-certificate>
        certificate:  <base64-encoded-certificate>
        privateKey:  <base64-encoded-private-key>
```

=== Gossip Encryption Key
Nomad requires a key if you want to encryption communication between nomad machines. This must be exactly 32 bytes long. Based on how you prefer to manage secrets there are 2 options.

*Option 1 - Create the secret yourself*
```
kubectl create secret generic nomad-gossip-encryption-key \
--from-literal=gossip-key=<secret-key-32-chars>
```

*Option 2 - CircleCI creates the secret*
Add the value to values.yaml, and CircleCI will create the secret automatically.
```
nomad:
  server:
    gossip:
      encryption:
        key: <secret-key-32-chars>
        ...
```
=== Nomad Autoscaler

If you have enabled the nomad autoscaler also include the following section under `nomad`.

```
  auto_scaler:
    enabled: true
    scaling:
      max: <max-node-limit>
      min: <min-node-limit>
```

=== AWS
the values for this section were created <<nomad-autoscaler-aws,here>>

```
  auto_scaler:
    enabled: true
    scaling:
      max: <max>
      min: <min>

    aws:
      enabled: true
      region: <region>
      autoScalingGroup: <asg-name>

      accessKey: <access-key>
      secretKey: <secret-key>
      # or
      irsaRole: <role-arn>
```

=== GCP
the values for this section were created <<nomad-autoscaler-gcp,here>>
```
    gcp:
      enabled: true
      project_id: <project-id>
      mig_name: <instance-group-name>

      region: <region>
      # or
      zone: <zone>

      service_account: <service-account-json>
      # or 
      workloadIdentity: <service-account-email>
```

=== Nomad Clients Validation

CircleCI has created a project called https://github.com/circleci/realitycheck/tree/server-3.0[realitycheck] which allows you to test your Server installation. We are going to follow the project so we can verify that the system is working as expected. As you continue through the next phase, sections of realitycheck will move from red to green.

To run realitycheck, you need to clone the repository. Depending on your GitHub setup, you can use one of the following commands:

==== GitHub Cloud

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
----

==== GitHub Enterprise

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
git remote set-url origin <YOUR_GH_REPO_URL>
git push
----

Once you have successfully cloned the repository, you can follow it from within your CircleCI server installation. You need to set the following variables. For full instructions please see the https://github.com/circleci/realitycheck/tree/server-3.0[repository readme].

.Environmental Variables
[.table.table-striped]
[cols=2*, options="header", stripes=even]
|===
|Name
|Value

|CIRCLE_HOSTNAME
|<YOUR_CIRCLECI_INSTALLATION_URL>

|CIRCLE_TOKEN
|<YOUR_CIRCLECI_API_TOKEN>
|===

.Contexts
[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|Name
|Environmental Variable Key
|Environmental Variable Value

|org-global
|CONTEXT_END_TO_END_TEST_VAR
|Leave blank

|individual-local
|MULTI_CONTEXT_END_TO_END_VAR
|Leave blank
|===

Once you have configured the environmental variables and contexts, rerun the realitycheck tests. You should see the features and resource jobs complete successfully. Your test results should look something like the following:

image::realitycheck-pipeline.png[Screenshot showing the realitycheck project building in the CircleCI app]

== VM service

VM service configures VM and remote docker jobs. You can configure a number of options for VM service, such as scaling rules. VM service is unique to AWS and GCP installations because it specifically relies on features of these cloud providers.

ifndef::env-gcp[]

=== AWS
. *Get the Information Needed to Create Security Groups*
+
The following command returns your VPC ID (`vpcId`), CIDR Block (`serviceIpv4Cidr`), Cluster Security Group ID (`clusterSecurityGroupId`) and Cluster ARN (`arn`) values, which you need throughout this section:
+
```shell
aws eks describe-cluster --name=<your-cluster-name>
```

. *Create a security group*
+
Run the following commands to create a security group for VM service:
+
```shell
aws ec2 create-security-group --vpc-id "<YOUR_VPCID>" --description "CircleCI VM Service security group" --group-name "circleci-vm-service-sg"
```
+
This outputs a GroupID to be used in the next steps:
+
[source, json]
{
    "GroupId": "sg-0cd93e7b30608b4fc"
}

. *Apply security group Nomad*
+
Use the security group you just created and CIDR block values to apply the security group to the following:
+
```shell
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 22 --cidr "<YOUR_serviceIpv4Cidr>"
```
+
```shell
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 2376 --cidr "<YOUR_serviceIpv4Cidr>"
```
+
NOTE: If you created your Nomad Clients in a different subnet from CircleCI server, you need to rerun the above two commands with each subnet CIDR.

. *Apply the security group for SSH*
+
Run the following command to apply the security group rules so users can SSH into their jobs:
+
```shell
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 54782
```

. *Create user*
+
Create a new user with programmatic access:
+
```shell
aws iam create-user --user-name circleci-vm-service
```
+
Optionally, vm-service does support the use of a https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[service account role] in place of AWS keys. If you would prefer to use a role, follow these https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[instructions] using the policy in step 6 below.
Once done, you may skip to step 9 which is enabling vm-service.
+
. *Create policy*
+
Create a `policy.json` file with the following content. You should fill in the ID of the VM Service security group created in step 2 (`VMServiceSecurityGroupId`) and VPC ID (`vpcID`) below.
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<YOUR_VMServiceSecurityGroupID>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<YOUR_vpcID>"
        }
      }
    }
  ]
}
----

. *Attach policy to user*
+
Once you have created the policy.json file, attach it to an IAM policy and created user.
+
```shell
aws iam put-user-policy --user-name circleci-vm-service --policy-name circleci-vm-service --policy-document file://policy.json
```

. *Create an access key and secret for the user*
+
If you have not already created them, you will need an access key and secret for the `circleci-vm-service` user. You can create those by running the following command:
+
```shell
aws iam create-access-key --user-name circleci-vm-service
```

. *Configure server*
+
Add the VM Service configuration to values.yaml. Details of the available configuration options can be found in the https://circleci.com/docs/2.0/server-3-operator-vm-service[VM Service] guide.
+

endif::env-gcp[]

ifndef::env-aws[]

=== GCP

You need additional information about your cluster to complete the next section. Run the following command:

```shell
gcloud container clusters describe
```

This command returns something like the following, which includes network, region and other details that you need to complete the next section:

[source, json]
----
addonsConfig:
  gcePersistentDiskCsiDriverConfig:
    enabled: true
  kubernetesDashboard:
    disabled: true
  networkPolicyConfig:
    disabled: true
clusterIpv4Cidr: 10.100.0.0/14
createTime: '2021-08-20T21:46:18+00:00'
currentMasterVersion: 1.20.8-gke.900
currentNodeCount: 3
currentNodeVersion: 1.20.8-gke.900
databaseEncryption:
…
----

. *Create firewall rules*
+
Run the following commands to create a firewall rule for VM service in GKE:
+
```shell
gcloud compute firewall-rules create "circleci-vm-service-internal-nomad-fw" --network "<network>" --action allow --source-ranges "0.0.0.0/0" --rules "TCP:22,TCP:2376"
```
+
NOTE: If you have used auto-mode, you can find the Nomad clients CIDR based on the region by referring to the https://cloud.google.com/vpc/docs/vpc#ip-ranges[table here].
+
```shell
gcloud compute firewall-rules create "circleci-vm-service-internal-k8s-fw" --network "<network>" --action allow --source-ranges "<clusterIpv4Cidr>" --rules "TCP:22,TCP:2376"
```
+
```shell
gcloud compute firewall-rules create "circleci-vm-service-external-fw" --network "<network>" --action allow --rules "TCP:54782"
```

. *Create user*
+
We recommend you create a unique service account used exclusively by VM Service. The Compute Instance Admin (Beta) role is broad enough to allow VM Service to operate. If you wish to make permissions more granular, you can use the Compute Instance Admin (beta) role documentation as reference.
+
```shell
gcloud iam service-accounts create circleci-server-vm --display-name "circleci-server-vm service account"
```
NOTE: If your are deploying CircleCI server in a shared VCP, you should create this user in the project in which you intend to run your VM jobs.

. *Get the service account email address*
+
```shell
gcloud iam service-accounts list --filter="displayName:circleci-server-vm service account" --format 'value(email)'
```

. *Apply role to service account*
+
Apply the Compute Instance Admin (Beta) role to the service account:
+
```shell
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/compute.instanceAdmin --condition=None
```
+
And
+
```shell
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/iam.serviceAccountUser --condition=None
```

. *Get JSON Key File*
+
If you are using link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identities] for GKE, this step is not required. 
+
After running the following command, you should have a file named `circleci-server-vm-keyfile` in your local working directory. You will need this when you configure your server installation.
+
```shell
gcloud iam service-accounts keys create circleci-server-vm-keyfile --iam-account <YOUR_SERVICE_ACCOUNT_EMAIL>
```

. *Enable Workload Identity for Service Account*
+
This step is required only if you are using link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identities] for GKE. Steps to enable Workload Identities are link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[here]
+
```shell
gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/vm-service]"
```

. *Configure Server*
+
Add the VM Service configuration to values.yaml. Details of the available configuration options can be found in the https://circleci.com/docs/2.0/server-3-operator-vm-service[VM Service] guide.
+


endif::env-aws[]

=== VM Service Validation

Once you have configured and deployed CircleCI server, you should validate that VM Service is operational. You can rerun the realitychecker project within your CircleCI installation and you should see the VM Service Jobs complete. At this point, all tests should pass.

== Runner

=== Overview

CircleCI runner does not require any additional server configuration. Server ships ready to work with runner. However, you need to create a runner and configure the runner agent to be aware of your server installation. For complete instructions for setting up runner, see the link:https://circleci.com/docs/2.0/runner-overview/?section=executors-and-images[runner documentation].

NOTE: Runner requires a namespace per organization. Server can have many organizations. If your company has multiple organizations within your CircleCI installation, you need to set up a runner namespace for each organization within your server installation.

ifndef::pdf[]

== What to read next

* https://circleci.com/docs/2.0/server-3-install-post[Server 3.x Phase 4 - Post installation]
* https://circleci.com/docs/2.0/server-3-install-hardening-your-cluster[Hardening Your Cluster]
* https://circleci.com/docs/2.0/server-3-install-migration[Server 3.x Migration]
endif::pdf[]
