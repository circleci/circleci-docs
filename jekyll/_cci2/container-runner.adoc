---
description: container runner
contentTags: 
  platform:
  - Cloud
---
= Container runner 
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

[#introduction-and-motivation]
== Introduction and motivation

CircleCI’s <<runner-overview#,self-hosted runner>> has historically executed each job using a one to one mapping between the CI job and a <<configuration-reference#machine,machine>> environment (virtual or physical) that has the self-hosted runner binary installed on it. By exclusively running jobs in this manner, users sacrifice several benefits of a container-based solution that are afforded on CircleCI’s cloud platform when using the <<using-docker#,Docker executor>>:

* The ability to seamlessly define, publish, and use custom Docker images during job execution.
* The ability to easily manage dependencies or libraries through custom Docker images instead of enumerating dependencies as part of `.circleci/config.yml` steps.
* Access to a consistent, clean containerized build environment with every job 

Container runner is a type of self-hosted runner a user can install in a Kubernetes (k8s) cluster which enables them to run containerized CI jobs on self-hosted compute, similar to how jobs that use the native Docker executor run on CircleCI’s cloud platform.

Container runner is a complement to the machine runner, not a replacement.

After installation of the container-agent, the container runner will claim your containerized jobs, schedule them within an ephemeral pod, and execute the work within a container-based execution environment.

.Container-agent model - Many container-agent : task-agents
image::container-runner-model.png[Container-agent model]

[#running-container-runner-first-job]
== Running your container runner's first job

<<runner-installation#circleci-web-app-installation,Create>> a runner <<runner-concepts#namespaces-and-resource-classes,resource class>> and its associated resource class token. This can be done via the CircleCI web app (recommended) or the <<runner-installation-cli#,CircleCI CLI>>. 

[#preqrequisites]
=== Prerequisites

* Kubernetes 1.12+
* Helm 3.x
* A runner resource class token.
* It is assumed that the container runner is running in a Kubernetes namespace without any other workloads, as it is possible that the runner or garbage collection (GC) could delete jobs in the same namespace.
* The `checkout` step configures git to checkout over SSH. Ensure that your cluster is configured to allow outbound connections over port 22 if you wish to use it.

[#installing-the-helm-chart]
=== Install the Helm chart

NOTE: The location for the Helm chart has changed as of October 6, 2022. The previous Helm chart repository location will continue to be kept up to date until a date to be announced later.

Install the Helm chart with the release name `container-agent`:

* Run `helm repo add container-agent https://packagecloud.io/circleci/container-agent/helm`
* Run `helm repo update`
* Run `helm install container-agent container-agent/container-agent -n circleci` to install the chart.
 
This will deploy the container runner to the Kubernetes cluster as a release called `container-agent`. You can view the default values by running `helm show values circleci/container-agent` and override these using the `--values` or `--set name=value` flags on the install command below.

[#update-values.yaml]
=== Update values.yaml

To run a job with no custom configuration, add the following configuration to the Helm chart `values.yaml`. `MY_TOKEN` is your runner resource class token. Update `namespace/my-rc` with your namespace and runner resource class.


```yaml
agent:
  resourceClasses:
    namespace/my-rc:
      token: MY_TOKEN
```

[#trigger-a-job]
=== Trigger a job

Once you have installed the container runner within your cluster, create and trigger a CircleCI job that uses the Docker executor to validate the installation.

- Within your `circleci/config.yml` file, use the <<using-docker#,Docker executor syntax>> combined with the resource class that you have included in the `resourceClasses` section of your container runner installation. 
+
- Specifically, to route a job to be run using container runner within your cluster, update the resource class stanza to use the resource class that you created for container runner jobs.
+
```YAML
resource_class: <namespace>/<name-of-resource-class-created>
```

NOTE: **Do not** use an existing job that uses <<building-docker-images#,setup_remote_docker>> (see <<#building-container-images,Building container images>> section below for details).

Once your configuration file is updated, validate whether the job ran successfully by triggering it and ensuring a green build using the CircleCI web app. See the link:/docs/runner-faqs#container-runner-specific-faqs[Runner FAQ page] for a full sample config if you are starting from scratch.

[#resource-class-configuration-custom-pod]
=== Resource class configuration and custom task pod configuration

Container runner supports claiming and running tasks from multiple resource classes concurrently, as well as customization of the Kubernetes resources created to run tasks for a particular resource class. Configuration is provided by a map object in the Helm chart `values.yaml`.

Each resource class supports the following parameters:

- `token`: The runner resource class token used to claim tasks (**required**).
- Custom Kubernetes pod configuration for pods used to run CircleCI jobs.

The pod configuration takes all fields that a normal link:https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#debugging[Kubernetes pod does]. If service containers are used in a CircleCI job, the first `container` spec is used for all containers within the task pod. There is currently no way to provide a different container configuration between service containers and the main task container. 

The following fields will be overwritten by container runner to ensure correct task function, and expected CircleCI configuration behavior:

- `spec.containers[0].name`
- `spec.containers[0].container.image`
- `spec.containers[0].container.args`
- `spec.containers[0].container.command`
- `spec.containers[0].container.workingDir`
- `spec.restartPolicy`
- `metadata.name`
- `metadata.namespace`

Below is a full configuration example, containing two resource classes:

```yaml
agent:
  resourceClasses:  
    circleci-runner/resourceClass:
      token: TOKEN1
      metadata:
        annotations:
          custom.io: my-annotation
      spec:
        containers:
          - resources:
              limits:
                cpu: 500m
            volumeMounts:
              - name: xyz
                mountPath: /path/to/mount
        securityContext:
          runAsNonRoot: true
        imagePullSecrets:
          - name: my_cred
        volumes:
          - name: xyz
            emptyDir: {}
    
    circleci-runner/resourceClass2:
      token: TOKEN2
      spec: 
        imagePullSecrets:
          - name: "other"
```

[#custom-secret]
=== Custom token secret

Using the configuration described above provisions a Kubernetes secret containing your resource class tokens. In some circumstances, you may wish to provision your own secret, or you simply might not want to specify the tokens via helm. Instead, you can provision your own Kubernetes secret containing your tokens and specify its name in the `agent.customSecret` field.

The secret should contain a field for each resource class, using the resource class name as the key and the token as the value. Consider the following `resourceClasses` configuration:

```yaml
agent:
  resourceClasses:  
    circleci-runner/resourceClass:
      metadata:
        annotations:
          custom.io: <my-annotation>
    
    circleci-runner/resourceClass2:
```

The corresponding custom secret would have 2 fields:

```yaml
circleci-runner.resourceClass: <my-token>
circleci-runner.resourceClass2: <my-token-2>
```

Due to Kubernetes secret key character constraints, the `/` separating the namespace and resouce class name is replaced with a `.` character. Other than this, the name must exactly match the `resourceClasses` config to match the token with the correct configuration.

Even if there is no further pod configuration, the resource class must be present in `resourceClasses` as an emtpy map, as shown by `circleci-runner/resourceClass2` in the above config example.

[#parameters]
=== Helm Chart Parameters
 
The following are **CircleCI specific settings**:

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
| Parameter
| Description
| Default

| agent.runnerAPI
| Runner API URL
| https://runner.circleci.com

| agent.name
| A (preferably) unique name assigned to this particular `container-agent` instance. This name will appear in your Runner Inventory page in the CircleCI UI. If left unspecified, the name will default to the name of the deployment.
| `container-agent` (the name of the deployment)

| agent.resourceClasses *Default must be updated in order to run a job successfully*
| Resource class task configuration. See the "<<resource-class-configuration-custom-pod,Resource Class Configuration>>" section above.
| {}

| agent.customSecret
| A user provided Kubernetes containing resource class tokens. See the "<<custom-secret,Custom Token Secret>>" section above.
| ""

| agent.terminationGracePeriodSeconds
| Termination grace period during container runner shutdown
| 18300

| agent.maxRunTime
| Max task run time. This value should be shorter than the grace period above - See <<runner-config-reference/#runner-max_run_time#, docs>> for potential values
| 5h

| agent.maxConcurrentTasks
| Maximum number of tasks claimed/run concurrently
| 20

| agent.kubeGCEnabled 
| Option to enabled/disable garbage collection 
| true

| agent.kubeGCThreshold  
| Length of time pods can run before deleted by GC 
| 5h5m

| agent.constraintChecker.enable
| Whether to enable the constraint checker
| false

| agent.constraintChecker.threshold
| Number of failed checks before disabling resource class claim
| 3

| agent.constraintChecker.interval
| The constraint check interval
| 15m

|===

---

The following is for **Kubernetes object settings**. All settings prefixed with `agent` below are for the container runner pod itself, not the ephemeral pods where jobs are executed.

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
| Parameter
| Description
| Default

| nameOverride
| Override the chart name
| ""

| fullnameOverride
| Override the full generated name
| ""

| agent.replicaCount
| Number of container-agents to deploy. The recommendation is to leave this value at 1
| 1

| agent.image.registry
| Agent image registry
| ""

| agent.image.repository
| Agent image repository
| circleci/container-agent

| agent.pullPolicy
| Agent image pull policy
| Always

| agent.tag
| Agent image tag
| latest

| agent.pullSecrets
| link:https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/[Secret objects] container private registry credentials for the container runner pod itself, not the ephemeral pods that execute tasks
| []

| agent.matchLabels
| Match labels used on agent pods
| app: container-agent

| agent.podAnnotations
| Extra annotations added to agent pods
| {}

| agent.podSecurityContext
| Security context policies added to agent pods
| {}

| agent.containerSecurityContext
| Security context policies add to agent containers
| {}

| agent.resources
| Custom resource specifications for container runner pods
| {}

| agent.nodeSelector
| Node selector for agent pods 
| {}

| agent.tolerations
| Node tolerations for agent pods
| {}

| agent.tolerations
| Node tolerations for agent pods
| []

| agent.affinity
| Node affinity for agent pods
| {}

| serviceAccount.create
| Create a custom service account for the agent
| true

| rbac.create
| Create a Role and RoleBinding for the service account
| true
|===

Container runner needs the following Kubernetes permissions:

* Pods, Pods/Exec, Pods/Log
** Get
** Watch 
** List
** Create
** Delete
* Secrets
** List
** Create
** Delete
 
By default a `Role`, `RoleBinding` and service account are created and attached to the container runner pod, but if you customize these, the above are the minimum required permissions.

It is assumed that the container runner is running in a Kubernetes namespace without any other workloads. It is possible that the agent or garbage collection (GC) could delete pods in the same namespace.

[#garbage-collection]
== Garbage collection

Each container runner has a garbage collector which will ensure any pods and secrets with the label `app.kubernetes.io/managed-by=circleci-container-agent` left dangling in the cluster are removed. By default this will remove all jobs older than five hours and five minutes. This can be shortened or lengthened via the `agent.kubeGCThreshold` parameter. However, if you do shorten the garbage collection (GC) frequency, also shorten the max task run time via the `agent.maxRunTime` parameter to be a value smaller than the new GC frequency. Otherwise a running task pod could be removed by the GC.

Container runner will drain and restart cleanly when sent a termination signal. Container runner will not automatically attempt to launch a task that fails to start. This can be done in the CircleCI web app.

If the container runner crashes, there is no expectation that in-process or queued tasks are handled gracefully. 

[#constraint-validation]
== Constraint Validation

Container runner allows you to configure task pods with the full range of Kubernetes settings. This means pods can potentially be configured in a way which cannot be scheduled due to their constraints. To help with this, container runner has a constraint checker which periodically validates each resource class configuration against the current state of the cluster, to ensure pods can be scheduled. This prevents container runner claiming jobs which it cannot schedule which would then fail. 

If the constraint checker fails too many checks, it will disable claiming for that resource class until the checks start to pass again.

Currently the following constraints are checked against the cluster state:

* link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector[Node Selectors]
* link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodename[Node Name]
* link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodename[Node Affinity] - Only MatchExpressions are checked

As an example of how this works, consider the following resource class configuration:

```yaml
agent:
  resourceClasses:  
    circleci-runner/resourceClass:
      token: TOKEN1
      spec:
        nodeSelector:
          disktype: ssd
    
    circleci-runner/resourceClass2:
      token: TOKEN2
```

The first resource class has a node selector to ensure it is scheduled to nodes with an SSD. For some reason during operations the cluster no longer has any nodes with that label. The constraint checker will now fail checks for `circleci-runner/resourceClass` and will disable claiming jobs until it finds nodes with the correct label again. `circleci-runner/resourceClass2` claiming is not affected, the checks for different resource classes are independent of each other.

[#cost-and-availability]
== Cost and availability

Container runner jobs are eligible for <<persist-data#managing-network-and-storage-use,Runner Network Egress>>. This is in line with the existing pricing model for self-hosted runners, and will happen with close adherence to the rest of CircleCI’s network and storage billing roll-out. If there are questions, reach out to your point of contact at CircleCI.

The same plan-based offerings for self-hosted runner link:https://circleci.com/pricing/#comparison-table[concurrency limits] apply to the container runner. Final pricing and plan availability will be announced closer to the general availability of the offering.

[#building-container-images]
== Building container images

link:https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#use-docker-in-docker[Docker in Docker] is not recommended due to the security risk it can pose to your cluster.

To build container images in a container-agent job, a user may use:

1. A third-party tool like Buildah or Kaniko
2. Machine runner installed with Docker installed on it
3. CircleCI-hosted compute

Note: Third-party tools should be used at your own discretion.

While jobs that run with container-agent cannot use CircleCI's <<building-docker-images#,setup_remote_docker>> feature, it is possible to use a third-party tool to build Docker images in your container-agent job without using the Docker daemon.

You can see an example link:https://discuss.circleci.com/t/setup-remote-docker-on-container-runner/45629/11?u=sebastian-lerner[on our community forum] of how some users have successfully used Kaniko to build a container image.

Another option is to use a tool called link:https://github.com/containers/buildah[Buildah]. Buildah can be used in your `.circleci/config.yml` syntax:

```yaml
docker:
  - image: quay.io/buildah/stable
```

[#using-the-buildah-image]
=== Using the Buildah image

Buildah relies on the link:https://github.com/containers/fuse-overlayfs[fuse-overlay] program inside of the container, which means that a fuse device plugin must be configured in order to use it. `/dev/fuse` is required to use `fuse-overlayfs` inside of the container, as this option tells Buildah on the host to add `/dev/fuse` to the container for Buildah's use. Kubernetes has a device plugin system to enable secure sharing of host devices with pods.

To install the configuration `dev/fuse`, clone this link:https://github.com/kuberenetes-learning-group/fuse-device-plugin/blob/master/fuse-device-plugin-k8s-1.16.yml[repository] to where you are running Helm commands for your container-agent deployment. Then run:

```
kubectl create -f fuse-device-plugin-k8s-1.16.yml
```

You can confirm that this has been configured correctly by running `kubectl get daemonset -n kube-system` and confirming that `fuse-device-plugin-daemonset` is present and ready.

Once this device has been added, update the container-agent <<#resource-class-configuration-custom-pod,resource class configuration>>:

```yaml
resourceClasses: 
 <namespace>/<resourceClass>:
  token: <token>
   spec:
    containers:
     - resources:
        limits:
         github.com/fuse: 1
```

This will now let you run Buildah commands with container agent jobs and build containers:

```yaml
  docker-image:
    docker:
      - image: quay.io/buildah/stable
    resource_class: <namespace>/<resourceClass>
    steps:
      - checkout
      - run:
          name: sanity-test
          command: |
            buildah version
      - run:
          name: Building-a-container
          command: |
            buildah bud -f ./Dockerfile -t myimage:0.1
            buildah push myimage:tag
```

[#using-buildah-with-custom-images]
=== Using Buildah with custom images

You can also build your own custom image and include the installation of Buildah in your Dockerfile:

```
sudo yum install buildah
```

If you plan to use a CircleCI link:https://circleci.com/developer/images[convenience image], ensure you add the repository for installation to your job's `steps`:

```
sudo apt-get update
sudo apt-get install -y wget ca-certificates gnupg2
VERSION_ID=$(lsb_release -r | cut -f2)
echo "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /" | sudo tee /etc/apt/sources.list.d/devel-kubic-libcontainers-stable.list
curl -Ls https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/xUbuntu_$VERSION_ID/Release.key | sudo apt-key add -
sudo apt-get update
sudo apt install buildah -y
```

Additionally, set the isolation variable to default to `chroot`:

```
# Default to isolate the filesystem with chroot.
ENV BUILDAH_ISOLATION=chroot
```

You can then follow the same instructions as <<#using-the-buildah-image,Using the Buildah image>> above to add the fuse device plugin to the container-agent deployment and update your `.circleci/config.yml` file to use your custom images and build container images in those jobs.

[#limitations]
== Limitations

* The ability to rerun a job with SSH.
* Any known <<runner-overview#limitations,limitation>> for the existing self-hosted runner will continue to be a limitation of container agent.
* There is no support for container environments other than Kubernetes at this time.
* There is no support for the installation of container runner via the UI-based install flow in the web app, with the exception of creating a runner resource class that can be used with container runner.
* Container runner does not yet work on link:https://circleci.com/pricing/server/[CircleCI's server offering]
* <<building-docker-images#,`setup_remote_docker`>> as a command is not supported with container runner.  See <<#building-container-images,Building Container Images>>.

[#how-to-receive-technical-help]
== How to receive technical help

Contact your point of contact at CircleCI directly or comment on the link:https://discuss.circleci.com/t/a-more-scalable-container-friendly-self-hosted-runner-container-agent-now-in-open-preview/45094[Discuss post].

[#faqs]
== FAQs

Please visit the <<runner-faqs#container-runner-specific-faqs,runner FAQ page>> to see commonly asked questions about container runner.

