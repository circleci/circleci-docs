---
version:
- Server v3.x
- Server Admin
---
= CircleCI Server v3.x - Installation
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

Before you begin with the CircleCI Server v3.x installation process, ensure all xref:server-3-install-prerequisites.adoc[prerequisites] are met.

toc::[]

## Step 1 - Install KOTs

CircleCI Server v{serverversion} uses KOTs from Replicated to manage and distribute Server v{serverversion}.
Ensure you are running the minimum KOTs (KOTs {kotsversion}) by running `kubectl kots version`.

NOTE: The KOTs command will open up a tunnel to the admin console. If running on Windows inside WSL2 the port is not available
on the host machine. Turning WSL off and back on should resolve the issue. For more information please see https://github.com/microsoft/WSL/issues/4199.

Next, run:

[source,bash]
----
kubectl kots install circleci-server
----

You will be prompted for a:

* namespace for the deployment
* password for the KOTs admin console

When complete, you should be provided with a URL to access the admin console. Usually this is http://localhost:8800.

If you need to get back to the management console at a later date you can run:

[source,bash]
----
kubectl kots admin-console -n <namespace kots was installed in>
----

## Step 2 - Configure Server (Part 1)

Once you have accessed the URL to the administrative console and uploaded your license file, you will see similar to the following,
where you will need to configure CircleCI Server.

.Server v{serverversion} Configuration
image::server-config.png[Server v{serverversion} Configuration]

### Global Settings
Global configuration settings are a collection of cross-cutting configuration settings that affect the function of the entire
application as a whole. Component-specific settings can be found below under the *Component Settings* section.

. *Domain Name* (required): The domain name for your server installation. For example, circleci.yourcompany.com. You will
need to configure this domain and point it to the external Frontend Load Balancer once it is configured in a later step.
. *Frontend TLS Private Key*: A PEM encoded TLS private key for the web application. A default TLS key is provided.
. *Frontend TLS Certificate*: A PEM encoded TLS certificate for the web application. A default TLS certificate is provided.

TIP: if you don’t already have a TLS certificate for your installation, you can generate one using a tool of your choice. See the <<Optional: Generate a TLS Certificate>> section.

[start=4]
. *The URI of your VM Service load balancer* (required): If you do not yet have this value, you can deploy the application with any value and then retrieve and update it in <<Step 4 - Obtain Load Balancer IPs>>.
. *Output Processor Load Balancer URI* (required): The URI of your Output Processor load balancer. If you do not yet have this value, you can deploy the application with any value and then retrieve and update it in <<Step 4 - Obtain Load Balancer IPs>>.
. *Nomad Load Balancer URI* (required): The URI of your Nomad load balancer. If you do not yet have this value, you can deploy the application with any value and then retrieve and update it in <<Step 4 - Obtain Load Balancer IPs>>.

#### Optional: Generate a TLS Certificate
One example of generating a TLS certificate is provided below. Assuming you are using Google DNS to create your CircleCI subdomain, a free option is to use LetsEncrypt via certbot and run the following command:

[source,bash]
----
sudo certbot certonly --dns-google --dns-google-credentials ${GOOGLE_APPLICATION_CREDENTIALS} -d
${DOMAIN}
----

This will create a private key and full chain in `/etc/letsencrypt/live/${DOMAIN}/privkey.pem`.

NOTE: The domain you intend to use for the certificate must point to the Kubernetes external load balancer IP for the frontend service, NOT the cluster IP itself.

### Encryption
These keysets are used to encrypt and sign artifacts generated by CircleCI.

. *Artifact Signing Key* (required): To generate, run: +
[source,bash]
----
docker run circleci/server-keysets:latest generate signing -a stdout
----
Copy and paste the entirety of the output into the Artifact Signing Key field.

[start=2]
. *Encryption Signing Key* (required) : To generate, run:
[source,bash]
----
docker run circleci/server-keysets:latest generate encryption -a stdout
----
Copy and paste the entirety of the output into the Encryption Signing Key field.

WARNING: It is recommended to store these securely; if they are lost, job history and artifacts will not be recoverable.

### GitHub
These settings control authorization to server using Github OAuth and allow for updates to Github using build status information.

. *Github Type*: Select Cloud or Enterprise.
. *OAuth Client ID* (required): In GitHub, navigate to *Settings* > *Developer Settings* > *OAuth Apps* and select the *Register a new application* button.

.Register a new OAuth application
image::github-oauth.png[GitHub OAuth ]

The domain you selected for your CircleCI installation is the Homepage URL and *<your-circle-ci-domain>/auth/github* is the Authorization callback URL.

[start=3]
. *OAuth Client Secret* (required): On your Oauth application, you can create one by selecting the *Generate a new client secret* button in GitHub.

NOTE: If using GitHub Enterprise, you will also need a personal access token and the domain name of your GitHub Enterprise instance. You must also enable HTTP API Rate Limiting from the management console.

### Object Storage
Server 3.0 hosts build artifacts, test results, and other state in object storage. Please choose the one that best suits your needs.

#### AWS S3
. *AWS S3* (required): Region your S3 bucket is in.
. *AWS S3 Bucket* (required): The name of your S3 bucket.
. *AWS IAM Access Key ID* (required): https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[AWS Access Key ID] for S3 bucket access.
. *AWS IAM Secret Key* (required): https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[IAM Secret Key] for S3 bucket access.

It is recommended to create a new user with programmatic access for this purpose. You should attach the following https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html[IAM policy] to the user:

[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:*"
      ],
      "Resource": [
        "arn:aws:s3:::<<Bucket Name>>",
        "arn:aws:s3:::<<Bucket Name>>/*"
      ]
    },
    {
      "Action": [
        "sts:GetFederationToken"
      ],
      "Resource": [
        "arn:aws:sts::<<AWS account ID>>:federated-user/build-*"
      ],
      "Effect": "Allow"
    }
  ]
}
----

#### Google Cloud Storage
(coming soon in GA)

#### Embedded Object Storage (coming soon)
Server 3.0 will utilize Minio for alternative object storage options. Minio provides a S3 like interface on top of numerous
object storage options. For more information on Minio please see https://min.io/.

### Email Notifications
Build notifications are sent via email.

. *Email Submission server hostname*: Host name of the submission server (e.g., for Sendgrid use smtp.sendgrid.net).
. *Username*: Username to authenticate to submission server. This is commonly the same as the user’s e-mail address.
. *Password*: Password to authenticate to submission server.
. *Port*: Port of the submission server. This is usually either 25 or 587. While port 465 is also commonly used for email submission,
it is often used with implicit TLS instead of StartTLS. Server only supports StartTLS for encrypted submission. +

WARNING: Outbound connections on port 25 are blocked on most cloud providers. Should you select this port, be aware that
your notifications may fail to send.

[start=5]
. *Enable StartTLS*: Enabling this will encrypt mail submission. +

WARNING: You should only disable this if you can otherwise guarantee the confidentiality of traffic.

### VM Service
VM Service configures VM and remote docker jobs. You can configure a number of options for VM service, such as scaling rules.

NOTE: We recommend that you leave these options at their defaults until you have successfully configured and verified your server installation.

#### AWS EC2
You will need the following fields to configure your VM Service to work with AWS EC2. Note that the Access Key and Secret
Key used by VM Service differs from the policy used by object storage in the previous section.

. *AWS Region* (required): This is the region the application is in.
. *AWS Windows AMI ID* (optional): If you require Windows builders, you can supply an AMI ID for them here.
. *Subnet ID* (required): Choose a subnet (public or private) where the VMs should be deployed.
. *Security Group ID* (required): This is the security group that will be attached to the VMs.

The recommended security group configuration can be found in the xref:server-3-install-hardening-your-cluster.adoc#external-vms[Hardening Your Cluster] section.

[start=5]
. *AWS IAM Access Key ID* (required): https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[AWS Access Key ID] for EC2 access.
. *AWS IAM Secret Key* (required): https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[IAM Secret Key] for EC2 access.

It is recommended to create a new user with programmatic access for this purpose. You should attach the following IAM policy to the user:

[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/${SECURITY_GROUP_ID}"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "${VPC_ARN}"
        }
      }
    }
  ]
}
----

[start=7]
. *Number of <VM type> VMs to keep prescaled*: Up to 5 VMs can be preallocate on standby to execute jobs. Set to 0 to have only on-demand VM provisioning.
lowers the start time for jobs and can improve performance. However, they will incur additional costs depending on your
environment. You can enter the number of desired VMs by type, from 0-5. While you can enter numbers greater than 5, this will not
have any effect.

[start=8]
. *VM Service Custom Configuration*: Custom configuration can fine tune many aspects of your VM service. This is an advanced option and we recommend you reach out to your account manager to learn more.

#### Google Cloud Platform
(coming soon in GA)

### Nomad
Configure aspects of your Nomad control plane here.

#### Enable Mutual TLS (mTLS)
mTLS encrypts and authenticates traffic between your Nomad control plane and Nomad clients. Choose to enable or disable this feature here.

WARNING: This should only be disabled if you can guarantee the authenticity of the nodes joining your cluster and confidentiality
of traffic from them to the control plane in some other way.

. *Nomad Server Certificate* (required if mTLS is enabled): Obtained in <<Step 3 - Install Nomad Clients>>. If you have not
yet reached Step 3, leave this disabled and modify it later.
. *Nomad Server Private Key* (required if mTLS is enabled): Obtained in <<Step 3 - Install Nomad Clients>>. If you have not
yet reached Step 3, leave this disabled and modify it later.
. *Nomad Server Certificate Authority (CA) Certificate* (required if mTLS is enabled): Obtained in <<Step 3 - Install Nomad Clients>>.
If you have not yet reached Step 3, leave this disabled and modify it later.

When all required information has been provided, click the *Continue* button and your CircleCI installation will be put
through a set of preflight checks to verify your cluster meets the minimum requirements. When completed successfully,
you should see something like the following:

.Sever v{serverversion} Preflight Checks
image::preflight-checks.png[Preflight Checks]

## Step 3 - Install Nomad Clients
As mentioned in the xref:server-3-overview.adoc[Overview], Nomad is a workload orchestration tool that CircleCI uses to schedule
(via Nomad Server) and run (via Nomad Clients) CircleCI jobs.

Nomad client machines are provisioned outside of the cluster and need access to the Nomad Control Plane, Output Processor,
and VM Service.

We curate Terraform modules to help install Nomad clients in your cloud provider of choice. You can browse the modules in
our https://github.com/CircleCI-Public/server-terraform[public repository].

### AWS
If you’d like to install Nomad clients in AWS, create a file `main.tf` file with the following contents:

[source,text]
----
# main.tf
terraform {
  required_version = ">= 0.14.0"
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = ">=3.0.0"
    }
  }
}
provider "aws" {
# Your region of choice here
region = "us-west-1"
}

module "nomad_clients" {
source = "git::https://github.com/CircleCI-Public/server-terraform.git?//nomad-aws?ref=3.0.0"

  # Number of nomad clients to run
  nodes = 4

  region = "<< Region you want to run nomad clients in >>"
  subnet = "<< ID of subnet you want to run nomad clients in >>"
  vpc_id = "<< ID of VPC you want to run nomad client in >>"

  server_endpoint = "<< hostname:port of nomad server >>"

  dns_server = "<< ip address of your VPC DNS server >>"
  blocked_cidrs = [
    "<< cidr blocks you’d like to block access to e.g 10.0.1.0/24 >>"
  ]
}

output "nomad_server_cert" {
value = module.nomad_clients.nomad_server_cert
}

output "nomad_server_key" {
value = module.nomad_clients.nomad_server_key
}

output "nomad_ca" {
value = module.nomad_clients.nomad_tls_ca
}
----

To deploy your Nomad clients simply run:

[source,bash]
----
terraform init
terraform plan
terraform apply
----

After Terraform is done spinning up the Nomad client(s) it will output the certificates and key needed for Nomad mTLS
encryption. Make sure to copy them somewhere safe, you will need them in the next step.

Once terraform apply is complete, click on the *Application* tab in the admin console and wait for the deployment Status
to show "Ready," then move on to the next step.

### Google Cloud Platform

If you’d like to to install Nomad clients in Google Cloud Platform, create a
file `main.tf`. An example is shared below to document common settings. For
documentation on all available variables please see
https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[the
module README].

[source,text]
----
# main.tf
provider "google-beta" {
  # Your specific credentials here
  project = "your-project"
  region  = "us-west1"
  zone    = "us-west1-a"
}
module "nomad_clients" {
  # Note the use of ref=<<tag>> to pin the version to a fixed release
  source = "git::https://github.com/CircleCI-Public/server-terraform.git?//nomad-gcp?ref=3.0.0"
  zone    = "us-west1-a"
  region  = "us-west1"
  network = "my-network"
  subnet  = "my-nomad-subnet"
  # hostname:port of Nomad load balancer
  server_endpoint = "nomad.example.com:4647"

  # Number of nomad clients to run
  min_replicas     = 3
  max_replicas     = 10
  # Example autoscaling policy: scale up if we ever reach 70% cpu utilization
  autoscaling_mode = "ONLY_UP"
  target_cpu_utilization = 0.70
  # Network policy example: block access to 1.1.1.1 from jobs and allow retry
  # with SSH from only 2.2.2.2
  blocked_cidrs = [
    "1.1.1.1/32"
  ]
  retry_with_ssh_allowed_cidr_blocks = [
    "2.2.2.2/32"
  ]
}
output "nomad_server_cert" {
  value = module.nomad_clients.nomad_server_cert
}
output "nomad_server_key" {
  value = module.nomad_clients.nomad_server_key
}
output "nomad_ca" {
  value = module.nomad_clients.nomad_tls_ca
}
----

To deploy your Nomad clients simply run:

[source,bash]
----
terraform init
terraform plan
terraform apply
----

After Terraform is done spinning up the Nomad client(s) it will output the
certificates and key needed for Nomad mTLS encryption. Make sure to copy them
somewhere safe, you will need them in the next step.

Once terraform apply is complete, click on the *Application* tab in the admin
console and wait for the deployment Status to show "Ready," then move on to the
next step.


### Optional: Running Jobs Outside the Nomad Client
CircleCI Server can run Docker jobs on Nomad clients, but it can also run jobs in a dedicated VM. These VM jobs are controlled by Nomad clients,
therefore the Nomad clients must be able to access the VM machines on port 22 for SSH and port 2376 for remote Docker jobs.

## Step 4 - Obtain Load Balancer IPs

Run `kubectl get services` and note the following service IP addresses. You will need these to finish configuring your installation.

* Frontend External
* VM Service Load Balance URI
* Output Processor Load Balancer URI
* Nomad Server Load Balancer URI

Depending on your cloud environment and configuration, your output can contain either an external IP address or a hostname
for your load balancers. Either will work.

The values for VM Service, Output Processor and Nomad Server should be added into the config as described in
<<Step 2 - Configure Server (Part 1)>>. The value from Frontend External should be used in <<Step 5 - Create a DNS Entry for the Frontend External Load Balancer>>
to create the DNS entry for your applications domain name.

If you had to leave the default value in place for the Nomad `server_endpoint` in the previous step, you can now go back
to the terraform repository, fill in the correct value in `terraform.tfvars` and run `terraform apply` again.

NOTE: At this time you can choose to create DNS entries for each of the load balancers. It is not required, but some users
prefer to do so. For example, VM service might be called vmservice.circleci.yourdomain.com.

## Step 5 - Create a DNS Entry for the Frontend External Load Balancer

Next, create a DNS entry for your external frontend load balancer, i.e. circleci.your.domain.com.

For more information on adding a new DNS record, see the following documentation:

- https://cloud.google.com/dns/docs/records#adding_a_record[Managing Records] (GCP)
- https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-creating.html[Creating records by using the Amazon Route 53 Console] (AWS)

## Step 6 - Configure Server (Part 2) and Deploy
Go back to the *Config* tab in the admin console.

TIP: Run `kubectl kots admin-console -n <namespace kots was installed in>` if you need to get back to the admin console.

### Global Settings
Enter the values obtained from <<Step 4 - Obtain Load Balancer IPs>> into VM Service Load Balancer URI, Output Processor
Load Balancer URI, and Nomad Load Balancer URI under Global Settings.

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
| Field
| External IP Example
| Value

| VM Service Load Balancer URI
| example.us-west-1.elb.amazonaws.com
| example.us-west-1.elb.amazonaws.com:3000

| Output Processor Load Balancer URI
| example.us-west-1.elb.amazonaws.com
| example.us-west-1.elb.amazonaws.com:8585

| Nomad Load Balancer URI
| example.us-west-1.elb.amazonaws.com
| example.us-west-1.elb.amazonaws.com:4647
|===

### Nomad
If mutual TLS (mTLS) is enabled, enter the Nomad Server Certificate, Nomad Server Private Key, and Nomad Server
Certificate Authority (CA) Certificate generated from <<Step 3 - Install Nomad Clients>>.

### Deploy
Click the *Save config* button to update your installation and re-deploy server.

## Step 7 - Validate Installation

. Launch your CircleCI installation in your browser, for example https://hostname.com.
. Sign up/Log in into your CircleCI installation. As the first user to log in, you will become the administrator at this point.
. Take a look at our https://circleci.com/docs/2.0/getting-started/#section=getting-started[Getting Started] guide to start adding projects.
. Use the https://github.com/circleci/realitycheck[CircleCI realitycheck] repository to check basic CircleCI functionality.

If you are unable to run your first builds successfully, start with the https://circleci.com/docs/2.0/troubleshooting[Troubleshooting]
guide for general troubleshooting topics, and the https://circleci.com/docs/2.0/nomad[Introduction to Nomad Cluster Operation]
for information about how to check the status of Nomad Clients within your installation.
