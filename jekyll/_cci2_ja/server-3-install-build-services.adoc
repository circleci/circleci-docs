---

version:
- Server v3.x
- サーバー管理者
---
= CircleCI Server v3.x インストール ステップ 3

:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

// This doc uses ifdef and ifndef directives to display or hide content specific to Google Cloud Storage (env-gcp) and AWS (env-aws). Currently, this affects only the generated PDFs. To ensure compatability with the Jekyll version, the directives test for logical opposites. For example, if the attribute is NOT env-aws, display this content. For more information, see https://docs.asciidoctor.org/asciidoc/latest/directives/ifdef-ifndef/.

CircleCI サーバー v3.x ビルドサービスのインストールステップを開始する前に、 <a data-type="xref" href="server-3-install-prerequisites.html">ステップ 1: 前提条件</a> と <a data-type="xref" href="server-3-install.html">ステップ 2: コアサービスのインストール</a>が実行済みであることを確認してください。

.インストール手順のフローチャート ステップ 3）
image::server-install-flow-chart-phase3.png[Flow chart showing the installation flow for server 3.x with phase 3 highlighted]

NOTE: 以下のセクションでは、`< >` の中に表示される認証情報の項目にご自身の情報を入力してください。


toc::[]

== ステップ 3: 実行環境のインストール

=== 出力プロセッサ

出力プロセッサは、 Normad クライアントからの出力処理を行います。 システムの速度が低下している場合にスケーリングするための重要なサービスです。 要求に応じてサービスをスケールアップできるよう、出力プロセッサのレプリカセットを増やすことをお勧めします。

KOTS の管理者コンソールにアクセスします。 名前空間を変更して `kubectl kots admin-console -n <YOUR_CIRCLECI_NAMESPACE>` を実行し、KOTS 管理者コンソールにアクセスします。



設定で次の項目を探して入力します。

. 
.*[Output Processor Load Balancer (出力プロセッサ ロードバランサー) のホストネーム]*: 次のコマンドにより、サービスの IP アドレスを取得します

+
kubectl get service output-processor --namespace=<YOUR_CIRCLECI_NAMESPACE>
. *[Save your configuration (設定を保存)]*:  Nomad クライアントの設定の完了後、設定のデプロイと確認を行います。

=== Nomad クライアント

https://circleci.com/docs/2.0/server-3-overview[概要]で述べたように、Nomad は CircleCI が CircleCI ジョブのスケジュール設定 (Nomad サーバー経由) と実行 (Nomad クライアント経由) に使用するワークロードオーケストレーションツールです。 

Nomad クライアントは Kubernetes クラスタの外部にインストールされ、コントロールプレーン（ Nomad サーバー）はクラスタ内にインストールされます。 Nomad クライアントと Nomad コントロールプレーン間の通信は、 mTLS によって保護されます。 Nomad クライアントのインストールが完了すると、 mTLS 証明書、プライベートキー、および認証局が出力されます。


完了すると、 CircleCI Server の設定を更新して、 Nomad コントロールプレーンが Nomad クライアントと通信できるようになります。

==== Terraform によるクラスタの作成

CircleCI では、任意のクラウド プロバイダーに Nomad クライアントをインストールできるように Terraform モジュールをキュレーションしています。 モジュールは、(<code>main.tf</code>) の AWS と GCP の両方の Terraform 設定ファイル（main.tf）例を含む、 https://github.com/CircleCI-Public/server-terraform[パブリックリポジトリ] で参照できます。 `main.tf` を完了するには、クラスタとサーバーのインストールに関する情報が必要です。 以下でその情報を入手する方法を説明します。

NOTE: Nomad Autoscaler もここでセットアップする場合は、この Terraform の設定にいくつかの要件を含められるため、本ガイドの <<#nomad-autoscaler-optional,Nomad Autoscaler>>  のセクションを参照してください。

// Don't include this section in the GCP PDF:

ifndef::env-gcp[]
===== AWS
クラスタおよびサーバのインストールに関する情報を Terraform 設定ファイル (`main.tf`) のフィールドに入力します。 変数の完全な例と完全なリストについては、 https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[こちら] を参照してください。

* *Server_endpoint*: Nomad サーバーエンドポイント（Nomad サーバー外部ロードバランサー の外部 IP アドレス）が必要です。 以下のコマンドで情報を入手します。
+
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
* クラスタの *Subnet ID (subnet)* 、 *VPC ID (vpcId)* 、 *DNS server (dns_server)*:
以下のコマンドを実行して、クラスタの VPC ID ((vpcId)、CIDR (serviceIpv4Cidr)、およびサブネット (サブネットの ID)を入手します。
+
aws eks describe-cluster --name=<YOUR_CLUSTER_NAME>
+
すると、以下のようなコマンドが返されます。
+
[source, json]
{...
"resourcesVpcConfig": {
    "subnetIds": [
        "subnet-033a9fb4be69",
        "subnet-04e89f9eef89",
        "subnet-02907d9f35dd",
        "subnet-0fbc63006c5f",
        "subnet-0d683b6f6ba8",
        "subnet-079d0ca04301"
    ],
    "clusterSecurityGroupId": "sg-022c1b544e574",
    "vpcId": "vpc-02fdfff4c",
    "endpointPublicAccess": true,
    "endpointPrivateAccess": false
...
"kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16"
        },
...
}
+
次に、見つけた VPCID を使用して次のコマンドを実行し、クラスタの CIDR ブロックを取得します。 AWS の場合、 DNS サーバーは CIDR ブロック (`CidrBlock`) の３番目の IP です。たとえば、CIDR ブロックが `10.100.0.0/16` の場合、 3 番目の IP は `10.100.0.2` になります。
+
aws ec2 describe-vpcs --filters Name=vpc-id,Values=<YOUR_VPCID>
+
すると、以下のようなコマンドが返されます。
+
[source, json]
{...
"CidrBlock": "192.168.0.0/16",
"DhcpOptionsId": "dopt-9cff",
"State": "available",
"VpcId": "vpc-02fdfff4c"
...}


適切な情報を入力したら `main.tf` ファイルのディレクトリから次のコマンドを実行することにより、Nomad クライアントをデプロイすることができます。

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

Terraform は、 Nomad クライアントのスピンアップが完了すると、 CircleCI Server で Nomad コントロールプレーンを設定するために必要な証明書とキーを出力します。 この情報は、安全な場所にコピーしてください。 この適用プロセスには 通常 1 分しかかかりません。

// Stop hiding from GCP PDF:
endif::[]

// Don't include this section in the AWS PDF:

ifndef::env-aws[]
===== GCP
You will need the IP address of the Nomad control plane (Nomad Server), which was created when you deployed CircleCI Server. 以下のコマンドを実行することで IP アドレスを取得できます。

[source,shell]
----
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
----

以下の情報も必要です。

* Nomad クライアントを実行する GCP プロジェクト
* Nomad クライアントを実行する GCP ゾーン
* Nomad クライアントを実行する GCP リージョン
* Nomad クライアントを実行する GCP ネットワーク
* Nomad クライアントを実行する GCP サブネットワーク

以下の例をローカル環境にコピーして、特定の設定に必要な情報を入力します。

variable "project" {
  type    = string
  default = "<your-project>"
}

variable "region" {
  type    = string
  default = "<your-region>"
}

variable "zone" {
  type    = string
  default = "<your-zone>"
}

variable "network" {
  type    = string
  default = "<your-network-name>"
  # if you are using a shared vpc, provide the network endpoint rather than the name. eg:
  # default = "https://www.googleapis.com/compute/v1/projects/<host-project>/global/networks/<your-network-name>"
}

variable "subnetwork" {
  type    = string
  default = "<your-subnetwork-name>"
  # if you are using a shared vpc, provide the network endpoint rather than the name. eg:
  # default = "https://www.googleapis.com/compute/v1/projects/<service-project>/regions/<your-region>/subnetworks/<your-subnetwork-name>"
}


variable "server_endpoint" {
  type    = string
  default = "<nomad-server-loadbalancer>:4647"
}

variable "nomad_auto_scaler" {
  type        = bool
  default     = false
  description = "If true, terraform will create a service account to be used by nomad autoscaler."
}

variable "enable_workload_identity" {
  type        = bool
  default     = false
  description = "If true, Workload Identity will be used rather than static credentials'"
}

variable "k8s_namespace" {
  type        = string
  default     = "circleci-server"
  description = "If enable_workload_identity is true, provide application k8s namespace"
}

provider "google-beta" {
  project = var.project
  region  = var.region
  zone    = var.zone
}


module "nomad" {
  source = "git::https://github.com/CircleCI-Public/server-terraform.git//nomad-gcp?ref=3.4.0"

  zone            = var.zone
  region          = var.region
  network         = var.network
  subnetwork      = var.subnetwork
  server_endpoint = var.server_endpoint
  machine_type    = "n2-standard-8"
  nomad_auto_scaler         = var.nomad_auto_scaler
  enable_workload_identity  = var.enable_workload_identity
  k8s_namespace             = var.k8s_namespace

  unsafe_disable_mtls    = true
  assign_public_ip       = true
  preemptible            = true
  target_cpu_utilization = 0.50
}

output "module" {
  value = module.nomad
}

適切な情報を入力したら、以下を実行して Normad クライアントをデプロイできます。

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

Terraform は、 Nomad クライアントのスピンアップが完了すると、 CircleCI Server で Nomad コントロールプレーンを設定するために必要な証明書とキーを出力します。 この情報は、安全な場所にコピーしてください。
endif::[]

==== Nomad Autoscaler

Nomad provides a utility to automatically scale up or down your Nomad clients, provided your clients are managed by a cloud provider's autoscaling resource. Nomad Autoscaler を使うと、自動スケーリングリソースとその場所を管理するための権限をユーティリティに与えるだけで済みます。 このリソースは KOTS 経由で有効化することができ、Nomad サーバーと Nomad Autoscaler サービスをデプロイします。 下記ではご自身のプロバイダーに Nomad Autoscaler を設定する方法を概説します。

NOTE: 自動スケーリンググループや管理対象のインスタンスグループを作成すると、最大および最小の Nomad クライアント数によって、対応する値セットが上書きされます。 It is recommended that you keep these values and those used in your Terraform the same so that the two don't compete.

If you do not require this service then feel free to jump to the *Save config* button to update your installation and redeploy server.

ifndef::env-gcp[]
===== AWS
. IAM ユーザーまたは Nomad Autoscaler のロールとポリシーを作成します。 You may take one of the following approaches:
  * Our link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[nomad module] creates an IAM user and outputs the keys if you set variable `nomad_auto_scaler = true`. 詳細については、リンクの例を参照してください。 If you've already created the clients, you can update the variable and run `terraform apply`. The created user's access key and secret will be available in Terraform's output.
  * You may also create a Nomad Autoscaler IAM user manually with the IAM policy below. その場合、ユーザーにアクセスキーとシークレットキーを生成する必要があります。
  * You may create a https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[Role for Service Accounts] for Nomad Autoscaler and attach the following IAM policy:

+

[source, json]
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:CreateOrUpdateTags",
                "autoscaling:UpdateAutoScalingGroup",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "<<Your Autoscaling Group ARN>>"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeScalingActivities",
                "autoscaling:DescribeAutoScalingGroups"
            ],
            "Resource": "*"
        }
    ]
}
. In your KOTS Admin Console, set Nomad Autoscaler to `enabled`
. 最大の Node 数を設定します* : ASG の最大値として現在設定されている値を上書きします。
 この値と Terraform で設定された値を変えないことをお勧めします。
. 最小の Node 数を設定します* : ASG の最小値として現在設定されている値を上書きします。
 この値と Terraform で設定された値を変えないことをお勧めします。
. Select cloud provider: `AWS EC2`
. Add the region of the autoscaling group
. You can chose one of the following:
.. Add the Nomad Autoscaler user's access key and secret key
.. Or, the Nomad Autoscaler role's ARN
. Nomad クライアントが作成された自動スケーリンググループの名前を追加します。
endif::[]

ifndef::env-aws[]
===== GCP
. Create a service account for Nomad Autoscaler
  * Our link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[nomad module] creates a service acount and outputs a file with the keys if you set the variables `nomad_auto_scaler = true` and `enable_workload_identity = false`. 詳細については、リンクの例を参照してください。 If you have already created the clients, simply update the variable and run `terraform apply`. The created user's key will be available in a file named `nomad-as-key.json`. If you are using GKE link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload Identities], set the variables `nomad_auto_scaler = true` and `enable_workload_identity = true`.
  * You may also create a nomad gcp service account manually. The service account will need the role `compute.admin`. It will also need the role `iam.workloadIdentityUser` if using link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload identities]
. Set Nomad Autoscaler to `enabled`
. Set Maximum Node Count*
. Set Minimum Node Count*
. Select cloud provider: `Google Cloud Platform`
. Add your Project ID
. Add Managed Instance Group Name
. Instance group type: link:https://cloud.google.com/compute/docs/instance-groups/#types_of_managed_instance_groups[Zonal or Regional].
. You can choose one of the following:
.. JSON of GCP service account for Nomad Autoscaler
.. Or, the Nomad Autoscaler Service Account Email Address if using link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identities]. Steps to enable Workload Identities on GCP cluster are link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[here].
.. Enable workload identity for `nomad-autoscaler` (kubernetes) service account

gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/nomad-autoscaler]"

NOTE: 静的 JSON 認証情報から Workload Identity に切り替える場合は、GCP および CircleCI KOTS 管理者コンソールからキーを削除する必要があります。
endif::[]

==== 設定とデプロイ

Nomad クライアントの導入が完了したら、 CircleCI Server と Nomad コントロールプレーンを設定できます。 KOTS の管理コンソールにアクセスします。 Get to the KOTS admin console by running the following command, substituting your namespace: `kubectl kots admin-console -n <YOUR_CIRCLECI_NAMESPACE>`

設定で次の項目を入力します。

* *Nomad Load Balancer (required)*
+
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
* *Nomad Server Certificate (required)* -
Provided in the output from `terraform apply`
* *Nomad Server Private Key (required)* -
Provided in the output from `terraform apply`
* *Nomad Server Certificate Authority (CA) Certificate (required)* -
Provided in the output from `terraform apply`
* *Build Agent Image* -
If you want to use a custom Docker registry to supply the CircleCI Build Agent, contact customer support for assistance.

Click the *Save config* button to update your installation and redeploy server.

==== Normad クライアントの確認

CircleCI has created a project called https://github.com/circleci/realitycheck/tree/server-3.0[realitycheck] which allows you to test your Server installation. CircleCIはこのプロジェクトをフォローし、システムが期待どおりに動作しているかを確認します。 引き続き次のステップを実行すると、 realitycheck のセクションが赤から緑に変わります。

realitycheck を実行するには、リポジトリのクローンを実行する必要があります。 Github の設定に応じて、以下のいずれかを実行します。

===== Github Cloud

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git

----

===== GitHub Enterprise

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
git remote set-url origin <YOUR_GH_REPO_URL>
git push
----

レポジトリのクローンに成功したら、CircleCI Server 内からフォローすることができます。 以下の変数を設定する必要があります。 For full instructions please see the https://github.com/circleci/realitycheck/tree/server-3.0[repository readme].

.環境変数
[.table.table-striped]
[cols=2*, options="header", stripes=even]
|===
|名前
|値

|CIRCLE_HOSTNAME
|<YOUR_CIRCLECI_INSTALLATION_URL>

|CIRCLE_TOKEN

|<YOUR_CIRCLECI_API_TOKEN>
|===

.コンテキスト
[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|名前
|環境変数キー
|環境変数値

|org-global
|CONTEXT_END_TO_END_TEST_VAR
|空欄のまま

|individual-local
|MULTI_CONTEXT_END_TO_END_VAR
|空欄のまま
|===

環境変数とコンテキストを設定したら、 realitycheck テストを再実行します。 機能とリソースジョブが正常に完了したことが表示されます。 テスト結果は次のようになります。


image::realitycheck-pipeline.png[Screenshot showing the realitycheck project building in the CircleCI app]

=== VM サービス

VM サービスは、VM とリモート Docker ジョブを設定します。 スケーリング ルールなど、さまざまなオプションを構成することができます。 VM サービスは、 EKS および GKE のインストールに固有のものです。これは、これらのクラウドプロバイダーの機能に特に依存しているためです。

ifndef::env-gcp[]
==== AWS
. *Get the Information Needed to Create Security Groups*

+
The following command will return your VPC ID (`vpcId`), CIDR Block (`serviceIpv4Cidr`), Cluster Security Group ID (`clusterSecurityGroupId`) and Cluster ARN (`arn`) values, which you will need throughout this section:

+

aws eks describe-cluster --name=<your-cluster-name>

. *Create a security group*
+
以下のコマンドを実行して、VM サービス用のセキュリティーグループを作成します。
+
aws ec2 create-security-group --vpc-id "<YOUR_VPCID>" --description "CircleCI VM Service security group" --group-name "circleci-vm-service-sg"
+
これにより次の手順で使用するグループ ID が出力されます。
+
[source, json]
{
    "GroupId": "sg-0cd93e7b30608b4fc"
}
. *Apply security group Nomad*
+
作成したセキュリティーグループと CIDR ブロック値を使ってセキュリティーグループを以下に適用します。
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 22 --cidr "<YOUR_serviceIpv4Cidr>"
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 2376 --cidr "<YOUR_serviceIpv4Cidr>"
+
NOTE: CircleCI Server とは異なるサブネットに Nomad クライアントを作成した場合は、サブネット CIDR ごとに上記の 2 つのコマンドを再実行する必要があります。
. *Apply the security group for SSH*
+
次のコマンドを実行してセキュリティグループルールを適用し、ユーザーがジョブに SSH 接続できるようにします。
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 54782
. *Create user*
+
プログラムでのアクセス権を持つ新規ユーザーを作成します。
+
aws iam create-user --user-name circleci-vm-service
+
Optionally, vm-service does support the use of a https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[service account role] in place of AWS keys. If you would prefer to use a role, follow these https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[instructions] using the policy in step 6 below.
完了したら、ステップ 9 に進みます。手順 9 では、KOTS で VM サービスを有効化します。
. *Create policy*
+
Create a `policy.json` file with the following content. You should fill in the ID of the VM Service security group created in step 2 (`VMServiceSecurityGroupId`) and VPC ID (`vpcID`) below.
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<YOUR_VMServiceSecurityGroupID>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<YOUR_vpcID>"
        }
      }
    }
  ]
}
----
. *Attach policy to user*
+
policy.json ファイルを作成したら、IAM ポリティーと作成したユーザーにアタッチします。
+
aws iam put-user-policy --user-name circleci-vm-service --policy-name circleci-vm-service --policy-document file://policy.json
. *Create an access key and secret for the user*
+
If you have not already, you will need an access key and secret for the `circleci-vm-service` user. 以下のコマンドを実行し、作成することができます。
+
aws iam create-access-key --user-name circleci-vm-service
. *Configure server*
+
VM サービスをKOTS 管理者コンソールから設定します。 Details of the available configuration options can be found in the https://circleci.com/docs/2.0/server-3-operator-vm-service[VM Service] guide.
+
Once you have configured the fields, *save your config* and deploy your updated application.
endif::[]

ifndef::env-aws[]
==== GCP

以下のセクションを完了するにはクラスタに関する追加情報が必要です。 次のコマンドを実行します。

gcloud container clusters describe

このコマンドは、次のような情報を返します。この情報には、ネットワーク、リージョン、および次のセクションを完了するために必要なその他の詳細情報が含まれます。

[source, json]
----
addonsConfig:
  gcePersistentDiskCsiDriverConfig:
    enabled: true
  kubernetesDashboard:
    disabled: true
  networkPolicyConfig:
    disabled: true
clusterIpv4Cidr: 10.100.0.0/14
createTime: '2021-08-20T21:46:18+00:00'
currentMasterVersion: 1.20.8-gke.900
currentNodeCount: 3
currentNodeVersion: 1.20.8-gke.900
databaseEncryption:
…
----

. *Create firewall rules*
+
以下のコマンドを実行して、GKE の VM サービス用のファイヤーウォール ルールを作成します。
+
gcloud compute firewall-rules create "circleci-vm-service-internal-nomad-fw" --network "<network>" --action allow --source-ranges "0.0.0.0/0" --rules "TCP:22,TCP:2376"
+
NOTE: You can find the Nomad clients CIDR based on the region by referring to the https://cloud.google.com/vpc/docs/vpc#ip-ranges[table here] if you have used auto-mode.
+
gcloud compute firewall-rules create "circleci-vm-service-internal-k8s-fw" --network "<network>" --action allow --source-ranges "<clusterIpv4Cidr>" --rules "TCP:22,TCP:2376"
+
gcloud compute firewall-rules create "circleci-vm-service-external-fw" --network "<network>" --action allow --rules "TCP:54782"
. *Create user*
+
VM サービス専用の一意のサービス アカウントを作成することをお勧めします。 コンピューティング インスタンス管理者 (ベータ版) ロールは、VM サービスを運用するための広範な権限を持っています。 アクセス許可をより詳細に設定したい場合は、 コンピューティング インスタンス管理者 (ベータ版) ロールのドキュメントを参照してください。
+
gcloud iam service-accounts create circleci-server-vm --display-name "circleci-server-vm service account"
+
NOTE: CircleCI Server を共有 VCP にデプロイする場合は、 VM ジョブを実行するプロジェクトにこのユーザーを作成します。
. *Get the service account email address*
+
gcloud iam service-accounts list --filter="displayName:circleci-server-vm service account" --format 'value(email)'
. *Apply role to service account*
+
コンピューティング インスタンス管理者 (ベータ版) ロールをサービスアカウントに適用します。
+
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/compute.instanceAdmin --condition=None
+
さらに
+
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/iam.serviceAccountUser --condition=None
. *Get JSON Key File*
+
If you are using link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identities] for GKE, this step is not required.
+
After running the following command, you should have a file named `circleci-server-vm-keyfile` in your local working directory. サーバーインストールを設定する際に必要になります。
+
gcloud iam service-accounts keys create circleci-server-vm-keyfile --iam-account <YOUR_SERVICE_ACCOUNT_EMAIL>
. *Enable Workload Identity for Service Account*
+
This step is required only if you are using link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identities] for GKE. Steps to enable Workload Identities are link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[here]
+
gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/vm-service]"

NOTE: 静的 JSON 認証情報から Workload Identity に切り替える場合は、GCP および CircleCI KOTS 管理者コンソールからキーを削除する必要があります。

. *Configure Server*
+
VM サービスをKOTS 管理者コンソールから設定します。 Details of the available configuration options can be found in the https://circleci.com/docs/2.0/server-3-operator-vm-service[VM Service] guide.
+
Once you have configured the fields, *save your config* and deploy your updated application.
endif::[]

==== VM サービスの検証

CircleCI Server の設定が完了したら、VM サービスが適切に動作しているか確認する必要がありあます。 CircleCI Server 内で、realitycheckプロジェクトを再実行できます。 緑色であれば VM サービスジョブは完了してます。 この時点で、すべてのテストが緑色で合格しているはずです。

=== ランナーのバージョン

==== 概要

CircleCI のランナーには、追加のサーバー設定は不要です。 CircleCI Server はランナーと連携する準備ができています。 ただし、ランナーを作成し、CircleCI Server のインストールを認識するようにランナーエージェントを設定する必要があります。 For complete instructions for setting up runner see the link:https://circleci.com/docs/2.0/runner-overview/?section=executors-and-images[runner documentation].

NOTE: ランナーには各組織につき１つ名前空間が必要です。  CircleCI Server には複数の組織が存在する場合があります。 CircleCI Server 内に複数の組織が存在する場合、各組織につき１つランナーの名前空間を設定する必要があります。

ifndef::pdf[]
## What to read next

* https://circleci.com/docs/2.0/server-3-install-post[Server 3.x Phase 4 - Post installation]
* https://circleci.com/docs/2.0/server-3-install-hardening-your-cluster[Hardening Your Cluster]
* https://circleci.com/docs/2.0/server-3-install-migration[Server 3.x Migration]
endif::[]