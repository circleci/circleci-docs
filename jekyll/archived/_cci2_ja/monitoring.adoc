---
contentTags:
  platform:
  - Server v2.x
  - サーバー管理者
---
= インストールしたサーバーの監視
:page-layout: classic-docs
:page-liquid:
:page-description: ここでは、インストールした CircleCI Server v2.x を監視するためのメトリクスについて説明します。
:icons: font
:toc: macro
:toc-title:

WARNING: CircleCI Server version 2.x は、リリースのサポートが終了しています。 リリースがサポートされているバージョンへのアップグレードについては、お客様のアカウントチームにご相談ください。

ここでは、インストールした CircleCI Server v2.x を監視するためのメトリクスについて説明します。

toc::[]

== メトリクスの概要

メトリクスとは、監視と分析を目的として収集される技術的な統計データのことです。 このデータには、CPU やメモリの使用率などの基本情報のほか、実行済みビルド数や内部エラー数といった高度なデータも含まれます。 メトリクスを活用すると、以下のことが可能になります。

* インシデントや異常な動作をすばやく検出する
* コンピューティング リソースを動的にスケールする
* インフラ全体の問題をさかのぼって把握する

=== CircleCI Server におけるメトリクスの仕組み

CircleCI Server v2.x.では、メトリクス収集用の主要コンポーネントとして、 Telegraf を使用しています。 Telegraf は、CircleCI サービスによって生成されたメトリクスデータを Datadog や AWS CloudWatch などのデータ監視プラットフォームに転送するサーバーソフトウェアです。

.メトリクス
image::metrics.png[メトリクス名]

CircleCI Server v2.x では、以下のようにメトリクスが収集されます。

* サーバーの各コンポーネントが Services マシン上で動作する Telegraf コンテナにメトリクスデータを送信する。
* Telegraf がポート 8125/UDP をリッスンし、全コンポーネントからのデータを受信 (入力) した後、事前に設定されたフィルターを適用してデータを保持するか破棄するかを判断する。
* 一部のメトリクスタイプでは、Telegraf 内にデータが保持され、最大値、最小値、平均、標準偏差、合計などの統計データを定期的に計算する。
* 最後に、Telegraf が Serveces マシン上の標準出力や、Datadog や AWS CloudWatch などの設定されたシンク (出力) にデータを送信する。

Telegraf は複数の入出力タイプを同時に受け取ることができるため、管理者は 1 つの Telegraf インスタンスで複数のメトリクス データセットを収集し、Datadog と CloudWatch の両方に転送することが可能です。

== メトリクスの標準設定

以下のコマンドで、メトリクスの設定ファイルを確認します。

ifndef::pdf[{% raw %}]
```shell
sudo docker inspect --format='{{range .Mounts}}{{println .Source "->" .Destination}}{{end}}' telegraf | grep telegraf.conf | awk '{ print $1 }' | xargs cat

```
ifndef::pdf[{% endraw %}]

このファイルには 4 つの注目すべきブロックがあります (管理コンソールの設定によっては、いくつかのブロックがない場合もあります )。

* `\[[inputs.statsd]]` – Input configuration to receive metrics data through 8125/UDP (as discussed above)
* `\[[outputs.file]]` – Output configuration to emit metrics to stdout. 受け取ったメトリクスはすべて、Telegraf の Docker ログに表示されるように設定されています。 メトリクスの設定をデバッグする際に役立ちます。
* `\[[outputs.cloudwatch]]` – Output configuration to emit metrics to CloudWatch
* `\[[outputs.datadog]]` – Output configuration to emit metrics to Datadog

この設定ファイルは、Replicated (CircleCI Server v2.x を管理およびデプロイするためのサービス) によって自動的に生成され、管理もすべて Replicated が行います。 この標準の設定をカスタマイズしたい場合は、変更したいブロックを挿入*しない*ように Replicated を設定します。

CAUTION: ファイルを直接変更しないでください。 直接行った変更は、サービスの再起動などの特定のイベントでは Replicated によって破棄されます。 For example, if a customized `\[[inputs.statsd]]` block is added _without_ stopping automatic interpolation, you will encounter errors as Telegraf attempts to listen to `8125/UDP` twice, and the second attempt will fail with `EADDRINUSE`.

メトリクスのカスタマイズを行わない標準的な設定では、必要なのは上述の設定のみす。 `/etc/circleconfig/telegraf`の下にファイルを置いてメトリクスのカスタマイズを設定した場合、その設定はメインの設定に追加されます。つまり、メインの設定とカスタマイズファイルの内容がすべて連結されるのです。 メトリクスのカスタマイズに関する詳細は、 <<custom-metrics>> のセクションを参照してください。

== システム監視のメトリクス

AWS Cloudwatch または Datadog のいずれかへのメトリクス転送を有効にするには、 <<supported-platforms>> セクションで使用するサービスの手順に従います。 以下のセクションでは、CircleCI Server 環境で利用可能なメトリクスの概要を説明します。

=== VM サービスと Docker のメトリクス

VM サービスと Docker サービスのメトリクスは、メトリクスの収集とレポート用プラグイン駆動型サーバーエージェントである https://github.com/influxdata/telegraf[Telegraf] によって転送されます。

以下のメトリクスが有効化されています。

* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#cpu-time-measurements[CPU]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#metrics[ディスク]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#metrics[メモリ]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net/NET_README.md[ネットワーク]
* https://github.com/influxdata/telegraf/tree/master/plugins/inputs/docker#metrics[Docker]

=== Nomad ジョブのメトリクス

https://www.nomadproject.io/docs/telemetry/metrics.html#job-metrics[Nomad ジョブのメトリクス]は、Nomad サーバーエージェントによって有効化され、送信されます。 レポートされるメトリクスは、以下の 5 つです。

[.table.table-striped]
[cols=2*, options="header", stripes=even]
[cols="6,5"]
|===
|Metric
|説明

|`circle.nomad.server_agent.poll_failure`
|Nomad エージェントの最後のポーリングが失敗した場合は 1、成功した場合は 0 を返します。

|`circle.nomad.server_agent.jobs.pending`
|クラスター全体の保留中ジョブの総数を返します。

|`circle.nomad.server_agent.jobs.running`
|クラスター全体の実行中ジョブの総数を返します。

|`circle.nomad.server_agent.jobs.complete`
|クラスター全体の完了済みジョブの総数を返します。

|`circle.nomad.server_agent.jobs.dead`
|クラスター全体の停止中ジョブの総数を返します。
|===

Nomad メトリクスのコンテナが正常に動作している場合、標準出力や標準エラーには何も出力されません。 障害が発生すると、標準エラーにメッセージが出力されます。

=== CircleCI のメトリクス
_CircleCI Server v2. 18 からサポート_

[.table.table-striped]
[cols=2*, stripes=even]
[cols="5,6"]
|===
| `circle.backend.action.upload-artifact-error`
| アーティファクトのアップロードに失敗した回数をトラックします。

| `circle.build-queue.runnable.builds`
| システムを経由するビルドのうち実行可能と見なされているビルドの数をトラックします。

| `circle.dispatcher.find-containers-failed`
| 1.0 ビルドの数をトラックします。

| `circle.github.api_call`
| CircleCI が GitHub に対して実行している API 呼び出しの回数をトラックします。

| `circle.http.request`
| CircleCI のリクエストへの応答コードをトラックします。

| `circle.nomad.client_agent.*``
| Nomad クライアントのメトリクスをトラックします。

| `circle.nomad.server_agent.*`
| 存在する Nomad サーバーの数をトラックします。

| `circle.run-queue.latency`
| 実行可能なビルドが待機している時間をトラックします。

| `circle.state.container-builder-ratio`
| Builder ごとのコンテナの数をトラックします (1.0 のみ)。

| `circle.state.lxc-available`
| 利用可能なコンテナの数をトラックします (1.0 のみ)。

| `circle.state.lxc-reserved`
| 予約/使用中のコンテナの数をトラックします (1.0 のみ)。

| `circleci.cron-service.messaging.handle-message`
| `cron-service` によって処理される RabbitMQ メッセージのタイミングと数を通知します。

| `circleci.grpc-response`
| grpc システムが呼び出すシステムの待機時間をトラックします。
|===

// There are a couple of nomad metrics in this table... they should maybe be moved to the section above? ^^

// Taken out of table until told otherwise
//| `Circle.vm-service.vm.assigned-vm`
// | Tracks how many vm’s are in use.

// | `Circle.vm-service.vms.delete.status`
// | Tracks how many vm’s we’re deleting at a given moment.

// | `Circle.vm-service.vms.get.status`
// | TBD (Tracks how many vm’s we have?)

// | `Circle.vm-service.vms.post.status`
// | TBD
<<<

== サポート対象プラットフォーム

メトリクスと監視用に組み込まれているプラットフォームは、AWS CloudWatch と DataDog の２つです。 以下のセクションでは、それぞれの有効化と設定について説明します。

=== AWS CloudWatch

AWS CloudWatch を有効にするには、以下の作業を行ってください。

1. 管理コンソールの設定ページに移動します。 お客様の CircleCI の URL の代わりに下記 URL を使用します: `your-circleci-hostname.com:8800/settings#cloudwatch_metrics`

2. AWS CloudWatch Metrics の下で [Enable (有効にする)] をクリックして設定を開始します。
+
. Cloudwatch の有効化
image::metrics_aws_cloudwatch1.png[AWS CloudWatch]

==== AWS CloudWatch の設定

設定には、2つのオプションがあります。

* Services Box の [IAM Instance Profile (IAM インスタンスプロファイル)] を使用し、カスタムの領域と名前空間を設定する方法
+
.CloudWatchの領域と名前空間
image::metrics_aws_cloudwatch2a.png[Configuration IAM]

* カスタムの領域と名前空間と共に、AWS のアクセスキーとシークレットキーを使用する方法
+
.アクセスキーとシークレットキー
image::metrics_aws_cloudwatch2b.png[Configuration Alt]

設定の保存後、AWS CloudWatch コンソールに移動すると、メトリクスが転送されていることを*確認*できます。

=== DataDog

Datadogを有効にするには、以下の作業を行ってください。

// 1. Disable Telegraf - at this time both Datadog and Telegraf require port 8125
. 管理コンソールの設定ページに移動します。 お客様の CircleCI の ホスト名の代わりに下記 URL を使用します:  `your-circleci-hostname.com:8800/settings#datadog_metrics`

. Datadog Metrics の下で [Enable (有効にする)] をクリックして設定を開始します。
+
.Datadog メトリクスの有効化
image::metrics_datadog1.png[Enable DataDog]

. Datadog API キーを入力します。 Datadog のメトリクスサマリーに移動すると、メトリクスが転送されていることを確認できます。
+
.Datadog API キーの入力
image::metrics_datadog2.png[DataDog API Key]

== カスタムメトリクス

Telegraf の設定ファイルを使用したカスタムメトリクスにより、Replicated に Datadog や AWS Cloudwatch への標準メトリクスの転送を許可するよりも、より細かく制御することができます。

サーバーのメトリクスの基本設定には、基本的な使用の場合のみが想定されています。 メトリクスの扱い方をカスタマイズすると、以下の際に有益です。

* メトリクスデータをご希望のプラットフォーム (ご自身の InfluxDB インスタンスなど）に転送する。
* 特定のイベントを検出するために、追加のメトリクスを監視する。
* データ分析プラットフォームに送信するメトリクス数の削減（グロスオペレーションコストの削減）。

=== 1. 標準メトリクスの設定を無効にする

Disable Replicated's interpolation of the Telegraf configuration to fully customize [[inputs.statsd]] and outputs:

. 管理コンソールを開きます。
. *Settings* ページで、 *Custom Metrics* セクションに移動し、[Use custom telegraf metrics (Telegraf のカスタムメトリクスを使用する)]オプションを有効にします。
+
.カスタムメトリクス
image::custom_metrics.png[Custom Metrics]
. スクロールダウンして変更を保存し、サービスを再起動します。

NOTE: サービスの再起動の際にダウンタイムが発生します。 無効にした後は、Replicated の設定に関わらず、Datadog や CloudWatch への出力を手動で設定する必要があります。

=== 2. カスタム設定を作成する

これで Telegraf のすべてのサポート機能を実行する準備が整いました。 あとは Telegraf の有効な設定ファイルを入力するだけです。

. Services マシンに SSH で接続します。
. `/etc/circleconfig/telegraf/statsd.conf` に以下を追加します。
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "." namepass = []
```
. `namepass` の下に、受信したいメトリクスを追加します。以下の例では、上記のリストの上から４つのみを設定しています。 (その他の設定例は下記を参照してください)。
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "." namepass = [
            "circle.backend.action.upload-artifact-error",
            "circle.build-queue.runnable.builds",
            "circle.dispatcher.find-containers-failed",
            "circle.github.api_call"
          ]
```
. `sudo docker restart telegraf`を実行して、Telegraf コンテナを再起動します。

NOTE: 詳細な設定方法については、 https://github.com/influxdata/telegraf/blob/master/README.md[Telegraf の README] を参照してください。

[discrete]
==== Telegraph の設定例

[discrete]
===== シナリオ 1: 2 つの InfluxDB インスタンスに標準メトリクスを記録する

下記の例では、デフォルトのメトリクスを２つの InfluxDB インスタンスに記録します。一つは InfluxDB オンプレミスサーバー (`your-influx-db-instance.example.com`)、もう一つは https://cloud2.influxdata.com/[InfluxDB Cloud 2] です。

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." namepass = [
    "circle.backend.action.upload-artifact-error",
    "circle.build-queue.runnable.builds",
    "circle.dispatcher.find-containers-failed",
    "circle.github.api_call",
    "circle.http.request",
    "circle.nomad.client_agent.*",
    "circle.nomad.server_agent.*",
    "circle.run-queue.latency",
    "circle.state.container-builder-ratio",
    "circle.state.lxc-available",
    "circle.state.lxc-reserved",
    "circle.vm-service.vm.assigned-vm",
    "circle.vm-service.vms.delete.status",
    "circle.vm-service.vms.get.status",
    "circle.vm-service.vms.post.status",
    "circleci.cron-service.messaging.handle-message",
    "circleci.grpc-response"
  ]

[[outputs.influxdb]]
  url = "http://your-influx-db-instance.example.com:8086"
  database = "circleci"

[[outputs.influxdb_v2]]
  urls = ["https://us-central1-1.gcp.cloud2.influxdata.com"]
  token = "YOUR_TOKEN_HERE"
  organization = "circle@example.com"
  bucket = "circleci"
```

[discrete]
===== シナリオ 2: すべてのメトリクスを Datadog に記録する

標準設定では選択されたメトリクスしか扱えないため、Telegraf により廃棄されるメトリクスが多くあります。 JVM 統計やコンテナごとの CPU 使用率などの廃棄された高度なデータを受信したい場合は、namepass フィルタを外すことで、受信したすべてのメトリクスを保持することができます。 この例では、Datadog へのメトリクス送信を設定する方法も示しています。 前述したように、Replicated の設定に関わらず、Datadog への出力は手動で設定する必要があります。

CAUTION: このシナリオでは、大量のデータが発生します。

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." [[outputs.datadog]]
  apikey = 'YOUR_API_KEY_HERE'
```

[discrete]
===== シナリオ 3: 限られたメトリクスを CloudWatch に送る

AWS は、CloudWatch の料金をスカラーのシリーズごとに (つまり「平均」や「合計」のレベル) 請求します。 メトリクスのキー (例： `circle.run-queue.latency`) ごとに複数のフィールド (例：平均、最大値、最小値、合計) が計算され、冗長なフィールドもあるため、CloudWatch に送信するフィールドを選択することもできます。 This can be achieved by configuring `\[[outputs.cloudwatch]]` with `fieldpass`. You also may declare `\[[outputs.cloudwatch]]` multiple times to pick up multiple metrics, as illustrated below.

```
[[inputs.statsd]]
  # Accept all metrics at input level to 1) enable output configurations without thinking of inputs, and to 2) dump discarded metrics to stdout just in case.
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." [[outputs.cloudwatch]]
    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['circle.run-queue.latency']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['mean']

[[outputs.cloudwatch]]
    # Outputs can be specified multiple times.

    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['mem']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['available_percent']
```

== その他のヒント

`docker logs -f telegraf` を実行してログをチェックすることで、設定した出力に出力プロバイダー (influx など) がリストされているかどうかを確認できます。 また、お使いの CircleCI Server システムのすべてのメトリクスが特定の環境にタグ付けされるようにするには、設定ファイルに以下のコードを記載します。

```yaml
[global_tags]
Env="<staging-circleci>"
```

デフォルトの高度なインストール手順については、https://github.com/influxdata/influxdb#installation[InfluxDB のドキュメント]を参照してください。

CAUTION: 設定を変更した場合、CircleCI アプリケーションの再起動が必要となり、ダウンタイムが発生します。

// Extra Metics info not currently included
////
### Datadog Dashboard Configuration

This section shows you how to set up a Datadog dashboard for CircleCI metrics. We also provide descriptions of the metrics we currently support.

NOTE: CircleCI metrics are subject to change. The names of individual metrics may change, as well as their scope and monitoring options. Any changes will take place along with our usual release cycle and will be flagged up in our Changelog**

\newpage

#### The dashboard

Below is an image of our Datadog dashboard showing graphs for Make Workflow, Run queue, Time to complete Workflow, Count of Workflows completed by Status, and Build Service Latency.

![DataDog Dashboard](images/datadog-0.png)

#### JSON dashboard creation

The following JSON is for the dashboard shown above. You can use this to build the dashboard for your CircleCI Server installation:

\pagebreak

\tiny

```
{
   "notify_list":null,
   "description":"created by support@circleci.com",
   "template_variables":[

   ],
   "is_read_only":false,
   "id":"b44-4vy-w6r",
   "title":"Critical Path: Jobs",
   "url":"/dashboard/b44-4vy-w6r/critical-path-customer-builds",
   "created_at":"2018-10-25T07:28:08.108516+00:00",
   "modified_at":"2019-03-19T08:54:28.109067+00:00",
   "author_handle":"paulrobinson@circleci.com",
   "widgets":[
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.avg{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"warm",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.median{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"cool",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (mean/median) (ms)"
         },
         "id":380774989
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.95percentile{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (95th percentile - ms)"
         },
         "id":395803486
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"avg:circle.run_queue.latency.avg{platform:picard}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Run queue: Time to job started (avg) ms"
         },
         "id":381397080
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.avg{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               },
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.median{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow Mean/Median in ms (Success/Failure/Error)"
         },
         "id":395476806
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.95percentile{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow 95th percentile ms (Success/Failure/Error)"
         },
         "id":395804031
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.count{*} by {status}.as_count()",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Count of workflows completed by Status"
         },
         "id":393871870
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:builds_service.service.process_build.max{*}.rollup(max)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"avg:builds_service.service.process_build.median{*}.rollup(avg)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Build Service Latency (time to process a build)"
         },
         "id":3833057922780384
      }
   ],
   "layout_type":"ordered"
}
```

\normalsize

#### The Metrics

Following are descriptions of the specific metrics related to workflows, followed by dashboard screengrabs with those metrics highlighted:

`workflows_conductor.messaging.make_workflow.time_since_push.avg` (gauge)

* Average time from a trigger (GitHub hook) entering CircleCI and the workflow being created, shown in milliseconds.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median` (gauge): Median time to execute a workflow, shown in milliseconds.--->

<!--`workflows_conductor.execute_workflow.time_to_complete.avg` (gauge)

* Average time to execute a workflow, shown in milliseconds.

![workflows_conductor.messaging.make_workflow.time_since_push.avg (gauge) Average time to make a workflow](images/datadog-1.png)

<!---![workflows_conductor.execute_workflow.time_to_complete.median (gauge): Median time to execute a workflow, shown in milliseconds](images/datadog-2.png)--->

<!---[workflows_conductor.messaging.make_workflow.time_since_push.median (gauge): Median time to make a workflow, shown as millisecond](images/datadog-3.png)--->

<!--![workflows_conductor.execute_workflow.time_to_complete.avg (gauge): Average time to execute a workflow](images/datadog-4.png)

\pagebreak

## Monitoring Tasks

The following section describes actions to take when a threshold is exceeded for a monitored metric, for the Workflows, API-service, Nomad, or VM service.

### Workflows

#### Workflow message timing outliers

`workflows_conductor.engine_handler.messages.timing.95percentile`

**Notes/Actions**: This metric is a good indicator that work is proceeding in a timely manner. If timing threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check for exceptions from the workflows-conductor containers.

#### Number of messages received

`workflows_conductor.engine_handler.messages.timing.count`

**Notes/Actions**: This metric is a good indicator that work is flowing through the system. If message count drops to zero, complete the following steps:

1. Restart the `workflows-conductor` container
2. Check `workflows-conductor` logs. If logging isn't happening, restart
3. Check Github webhooks are being recieved to trigger jobs
4. Check for exceptions from `workflows-conductor` or `frontend` containers

#### Average time taken for Workflows to complete

`workflows_conductor.execute_workflow.time_to_complete.avg`

**Notes/Actions**: Some variation here is expected due to fluctuations in job and usage queue times. If threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check `domain-service` logs. If logging isn't happening, restart.
3. Check `contexts-service` logs. If logging isn't happening, restart.
4. Check `permissions-service` logs. If logging isn't happening, restart.
5. Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service` containers.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median`
Indicates TBD, if threshold is exceeded, complete the following steps:
1. TBD
2. TBD
3. TBD--->

<!--#### Workflows conductor memory used

`jvm.memory.total.used`

**Tag filter**: `service:workflows-conductor`

**Notes/Actions**: Indicates the amount of memory used by the Workflows Conductor service. If threshold is exceeded restart the `workflows-conductor`

\pagebreak

### API-service

The following metrics can be inspected to get diagnostic information on how the API service is running.

#### Average API response time

`backplane.ring.http_request.avg`

**Tag filter**: `service:api-service`

**Notes/ Actions**: Indicates the average response time from the API is increasing.

#### Number of API requests

`backplane.ring.http_request.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: Indicates a high number of API requests.

#### Maximum time to return an API response

`backplane.ring.http_request.max`

**Tag filter**: `service:api-service`

#### Slow API response speed

`backplane.ring.http_request.95percentile`

**Tag filter**: `service:api-service`

#### Number of active threads in the JVM

`jvm.thread.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: If this count goes above 1000, set `DOMAIN_SERVICE_REFRESH_USERS` environment variable to `false`.

#### GraphQL Resolver

`circleci.api_service.graphql.resolver.avg`

**Tag filter**: `service:api-service`

**Notes/Actions**: This metric can be split up using `type` tags to determine downstream service issues. If the threshold is exceeded across types, complete the following steps:

1. Take a thread dump of the api-service
2. Restart
3. Supply the thread dump with any tickets

If the slowdown is only for a subset of types, then inspect metrics for the corresponding service.

### Nomad

#### Average latency of builds in queue

`circle.run_queue.latency.avg`

**Notes/Actions**: Captures backup between CircleCI and Nomad. If threshold is exceeded, add additional capacity to Nomad or your VM pool.

## Monitor Settings

This section describes threshold settings for the Nomad, Domain, Workflows and VM Service to monitor common failure conditions and checks or corrective actions for each condition.

### Nomad

#### More than 10 recent jobs failed on {host}

`sum(last_10m):sum:build_agent.infra_failed{env:prod} by {host}.as_count() > 10`

**Notes/Actions**: This may indicate a bad host.

#### A number of builds are queued due to Nomad capacity

```
min(last_10m):avg:circle.run_queue.latency.avg /
{env:production,platform:picard} > 65000
```

**Notes/Actions**: Scale up the number of Nomad clients.

### Domain Service

#### Error rate increased

\footnotesize

```
avg(last_5m):default(sum:circle.domain_service.users.id.get.status{!status:200,!status:202}.as_count(), 0) /
default(sum:circle.domain_service.users.id.get.status{*}.as_count(), 0) >= 0.5
```
\normalsize

**Notes/Actions**: This might indicate problems with GitHub, check for exceptions in `domain-service` logs.

### Permissions Service

#### Error rate increased

\footnotesize

```
avg(last_5m):( default(sum:circle.permissions_service.permissions.get.status{status:500}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:502}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:503}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:504}.as_count(), 0) ) /
( default(sum:circle.permissions_service.permissions.get.status{status:200}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:202}.as_count(), 0) ) >= 0.2
```

\normalsize

**Notes/Actions**: This might indicate problems with `domain-service`, check for exceptions in `permissions-service` and `domain-service` logs.

### Workflows

#### gRPC error rate is elevated

```
avg(last_10m):sum:grpc_response.count /
{service:workflows-conductor,!status:ok}.as_count() /
sum:grpc_response.count{service:workflows-conductor}.as_count() > 0.2
```

**Notes/Actions**: Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service`.

#### No scheduled workflows have run in the last 5 minutes

```
sum(last_5m):sum:workflows_conductor.trigger.decision /
{decision:success}.as_count() < 1
```

**Notes/Actions**: Perform the following corrective actions:

1. Check `cron-service` logs. If logging isn't happening, restart.
2. Check for exceptions from `cron-service` and `workflows-conductor`.

### VM Service

#### VM service is responding with 5x errors
\footnotesize

```
sum(last_1m):sum:circle.vm_service.vms.get.status /
{status:500}.as_count() + /
sum:circle.vm_service.vms.get.status{status:503}.as_count() + /
sum:circle.vm_service.vms.get.status{status:504}.as_count() + /
sum:circle.vm_service.vms.post.status{status:500}.as_count() + /
sum:circle.vm_service.vms.post.status{status:504}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:500}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:503}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:504}.as_count() > 3
```
\normalsize

**Notes/Actions**: Check VM service metrics to identify root cause.

#### Multiple VM service provisioning errors

```
sum(last_10m):sum:build_agent.machine.created.count /
{result:error} by {resource_class_id}.as_count() > 50
```

**Notes/Actions**: This may be indicative of an issue like rate-limiting.

#### VM machine provisioning taking too long
\footnotesize

```
avg(last_5m):avg:build_agent.machine.created.avg /
{result:succeeded,resource_class_id:l1.medium, /
!docker_layer_caching:true} > 180000
```

\normalsize

**Notes/Actions**: Check VM service metrics to look for potential problems (this monitor could also be related to disk IOPS contention).-->
////
