---
version:
- Server v2.x
- Server Admin
---
= Monitoring Your Installation
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

This section includes information on metrics for monitoring your CircleCI Server installation.

toc::[]

== Metrics Overview

Metrics are technical statistical data collected for monitoring and analytics purposes. The data includes basic information, such as CPU or memory usage, as well as more advanced counters, such as number of executed builds and internal errors. Using metrics you can:

* Quickly detect incidents and abnormal behavior
* Dynamically scale compute resources
* Retroactively understand infrastructure-wide issues

=== How Metrics Work in CircleCI Server

Telegraf is the main component used for metrics collection in CircleCI Server. Telegraf is server software that brokers metrics data emitted by CircleCI services to data monitoring platforms such as Datadog or AWS CloudWatch.

.Metrics
image::metrics.png[Metrics]

Metrics collection in CircleCI Server works as follows:

* Each component of a Server installation sends metrics data to the telegraf container running on the Services machine.
* Telegraf listens on port 8125/UDP and receives data from all components (inputs) and applies configured filters to determine whether data should be kept or dropped.
* For some metric-types, Telegraf keeps metrics data inside and calculates statistical data (such as max, min, mean, stdev, sum) periodically.
* Finally, Telegraf sends out data to configured sinks (outputs), such as stdout (on the Services machine), Datadog and/or AWS CloudWatch.

It is worth noting that Telegraf can accept multiple input and output types at the same time allowing administrators to configure a single Telegraf instance to collect and forward multiple metrics data sets to both Datadog and CloudWatch.

== Standard Metrics Configuration

Review your metrics configuration file using the following command:

ifndef::pdf[{% raw %}]
```sh
sudo docker inspect --format='{{range .Mounts}}{{println .Source "->" .Destination}}{{end}}' telegraf | grep telegraf.conf | awk '{ print $1 }' | xargs cat
```
ifndef::pdf[{% endraw %}]

There are four notable blocks in the file (some blocks might not be there depending on your configuration in the Management Console):

* `\[[inputs.statsd]]` – Input configuration to receive metrics data through 8125/UDP (as discussed above)
* `\[[outputs.file]]` – Output configuration to emit metrics to stdout. All accepted metrics are configured to be shown in Telegraf docker logs. This is helpful for debugging your metrics configuration.
* `\[[outputs.cloudwatch]]` – Output configuration to emit metrics to CloudWatch
* `\[[outputs.datadog]]` – Output configuration to emit metrics to Datadog

This configuration file is automatically generated by Replicated (the service used to manage and deploy CircleCI Server) and is fully managed by Replicated. If you wish to customize the standard configuration you will need to configure Replicated to **not** insert the blocks you want to change. 

CAUTION: Do not attempt to directly modify the file. Any changes made in this way will be destroyed by Replicated upon certain events, such as service restarts. For example, if a customized `\[[inputs.statsd]]` block is added _without_ stopping automatic interpolation, you will encounter errors as Telegraf attempts to listen to `8125/UDP` twice, and the second attempt will fail with `EADDRINUSE`.

In a standard configuration with no metrics customization the main config discussed above is all that is required. If you have configured metrics customization by placing files under `/etc/circleconfig/telegraf`, those configurations are appended to the main config – imagine `cat`ing the main config and all of those customization files. For more on customizing metrics see the <<custom-metrics>> section.

== System Monitoring Metrics

To enable metrics forwarding to either AWS Cloudwatch or Datadog, follow the steps for the service you wish to use in the <<supported-platforms>> section. The following sections give an overview of available metrics for your installation.

=== VM Service and Docker Metrics

VM Service and Docker services metrics are forwarded via https://github.com/influxdata/telegraf[Telegraf], a plugin-driven server agent for collecting and reporting metrics.

The following metrics are enabled:

* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#cpu-time-measurements[CPU]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#metrics[Disk]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#metrics[Memory]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net/NET_README.md[Networking]
* https://github.com/influxdata/telegraf/tree/master/plugins/inputs/docker#metrics[Docker]

=== Nomad Job Metrics

https://www.nomadproject.io/docs/telemetry/metrics.html#job-metrics[Nomad job metrics] are enabled and emitted by the Nomad Server agent. Five types of metrics are reported:

[.table.table-striped]
[cols=2*, options="header", stripes=even]
[cols="6,5"]
|===
|Metric
|Description

|`circle.nomad.server_agent.poll_failure`
|Returns 1 if the last poll of the Nomad agent failed, otherwise it returns 0.

|`circle.nomad.server_agent.jobs.pending`
|Returns the total number of pending jobs across the cluster.

|`circle.nomad.server_agent.jobs.running`
|Returns the total number of running jobs across the cluster.

|`circle.nomad.server_agent.jobs.complete`
|Returns the total number of complete jobs across the cluster.

|`circle.nomad.server_agent.jobs.dead`
|Returns the total number of dead jobs across the cluster.
|===

When the Nomad metrics container is running normally, no output will be written to standard output or standard error. Failures will elicit a message to standard error.

=== CircleCI Metrics
_Introduced in CircleCI Server v2.18_

[.table.table-striped]
[cols=2*, stripes=even]
[cols="5,6"]
|===
| `circle.backend.action.upload-artifact-error`
| Tracks how many times an artifact has failed to upload.

| `circle.build-queue.runnable.builds`
| Tracks how many builds flowing through the system are considered runnable.

| `circle.dispatcher.find-containers-failed`
| Tracks how many 1.0 builds

| `circle.github.api_call`
| Tracks how many api calls CircleCI is making to github

| `circle.http.request`
| Tracks the response codes to CircleCi requests

| `circle.nomad.client_agent.*``
| Tracks nomad client metrics

| `circle.nomad.server_agent.*`
| Tracks how many nomad servers there are.

| `circle.run-queue.latency`
| Tracks how long it takes for a runnable build to be accepted.

| `circle.state.container-builder-ratio`
| Keeps track of how many containers exist per builder ( 1.0 only ).

| `circle.state.lxc-available`
| Tracks how many containers are available ( 1.0 only )

| `circle.state.lxc-reserved`
| Tracks how many containers are reserved/in use ( 1.0 only ).

| `circleci.cron-service.messaging.handle-message`
| Provides timing and counts for RabbitMQ messages processed by the `cron-service`

| `circleci.grpc-response`
| Tracks latency over the system grpc system calls.
|===

// There are a couple of nomad metrics in this table... they should maybe be moved to the section above? ^^

// Taken out of table until told otherwise
//| `Circle.vm-service.vm.assigned-vm`
// | Tracks how many vm’s are in use.

// | `Circle.vm-service.vms.delete.status`
// | Tracks how many vm’s we’re deleting at a given moment.

// | `Circle.vm-service.vms.get.status`
// | TBD (Tracks how many vm’s we have?)

// | `Circle.vm-service.vms.post.status`
// | TBD
<<<

== Supported Platforms

We have two built-in platforms for metrics and monitoring: AWS CloudWatch and DataDog. The sections below detail enabling and configuring each in turn.

=== AWS CloudWatch

To enable AWS CloudWatch complete the following:

1. Navigate to the settings page within your Management Console. You can use the following URL, substituting your CircleCI URL: `your-circleci-hostname.com:8800/settings#cloudwatch_metrics`.

2. Check Enabled under AWS CloudWatch Metrics to begin configuration.
+
.Enable Cloudwatch
image::metrics_aws_cloudwatch1.png[AWS CloudWatch]

==== AWS CloudWatch Configuration

There are two options for configuration:

* Use the IAM Instance Profile of the services box and configure your custom region and namespace.
+
.CloudWatch Region and Namespace
image::metrics_aws_cloudwatch2a.png[Configuration IAM]

* Alternatively, you may use your AWS Access Key and Secret Key along with your custom region and namespace.
+
.Access Key and Secret Key
image::metrics_aws_cloudwatch2b.png[Configuration Alt]

After saving you can *verify* that metrics are forwarding by going to your AWS CloudWatch console.

=== DataDog

To enable Datadog complete the following:

// 1. Disable Telegraf - at this time both Datadog and Telegraf require port 8125
. Navigate your Management Console Settings. You can use the following URL, substituting your CircleCI hostname: `your-circleci-hostname.com:8800/settings#datadog_metrics`

. Check Enabled under Datadog Metrics to begin configuration.
+
.Enable Datadog Metrics
image::metrics_datadog1.png[Enable DataDog]

. Enter your DataDog API Key. You can verify that metrics are forwarding by going to your DataDog metrics summary.
+
.Enter Datadog API key
image::metrics_datadog2.png[DataDog API Key]

== Custom Metrics

Custom Metrics using a Telegraf configuration file allows for more fine grained control than allowing Replicated to forward standard metrics to Datadog or AWS Cloudwatch.

The basic Server metrics configuration assumes fundamental use cases only. It might be beneficial to customize the way metrics are handled for your installation in the following ways:

* Forward metrics data to your preferred platform (e.g. your own InfluxDB instance)
* Monitor additional metrics in order to detect specific events
* Reduce the number of metrics sent to data analysis platforms (to reduce gross operation costs)

=== 1. Disable Standard Metrics Setup

Disable Replicated's interpolation of the Telegraf configuration to fully customize [[inputs.statsd]] and outputs:

. Open the Management Console.
. On the **Settings** page, go to **Custom Metrics** section and enable the "Use custom telegraf metrics" option.
+
.Custom Metrics
image::custom_metrics.png[Custom Metrics]
. Scroll down to save the change and restart services.

NOTE: There will be a downtime along with a service restart. After disabling it you will have to manually configure outputs to Datadog and/or CloudWatch, regardless of configurations on Replicated.

=== 2. Create your Customized Config

Now you are ready to do anything Telegraf supports! All you need to provide is a valid Telegraf config file.

. SSH into the Services machine
. Add the following to `/etc/circleconfig/telegraf/statsd.conf`
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "." namepass = []
```
. Under `namepass` add any metrics you wish to receive, the example below shows choosing to configure just the first 4 from the list above. (See below for some additional example configs):
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "." namepass = [
            "circle.backend.action.upload-artifact-error",
            "circle.build-queue.runnable.builds",
            "circle.dispatcher.find-containers-failed",
            "circle.github.api_call"
          ]
```
. Restart the telegraf container by running: `sudo docker restart telegraf`

NOTE: See the https://github.com/influxdata/telegraf/blob/master/README.md[Telegraf README] for further config syntax details.

[discrete]
==== Sample Telegraph Configuration

[discrete]
===== Scenario 1: Record standard metrics to two InfluxDB instances

The example below records default metrics to two InfluxDB instances: One is your on-premises InfluxDB server (`your-influx-db-instance.example.com`), and the other is https://cloud2.influxdata.com/[InfluxDB Cloud 2].

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." namepass = [
    "circle.backend.action.upload-artifact-error",
    "circle.build-queue.runnable.builds",
    "circle.dispatcher.find-containers-failed",
    "circle.github.api_call",
    "circle.http.request",
    "circle.nomad.client_agent.*",
    "circle.nomad.server_agent.*",
    "circle.run-queue.latency",
    "circle.state.container-builder-ratio",
    "circle.state.lxc-available",
    "circle.state.lxc-reserved",
    "circle.vm-service.vm.assigned-vm",
    "circle.vm-service.vms.delete.status",
    "circle.vm-service.vms.get.status",
    "circle.vm-service.vms.post.status",
    "circleci.cron-service.messaging.handle-message",
    "circleci.grpc-response"
  ]

[[outputs.influxdb]]
  url = "http://your-influx-db-instance.example.com:8086"
  database = "circleci"

[[outputs.influxdb_v2]]
  urls = ["https://us-central1-1.gcp.cloud2.influxdata.com"]
  token = "YOUR_TOKEN_HERE"
  organization = "circle@example.com"
  bucket = "circleci"
```

[discrete]
===== Scenario 2: Record all metrics to Datadog

The standard configuration handles only selected metrics, and there are many metrics discarded by Telegraf. If you want to receive this discarded, sophisticated data, such as JVM stats and per-container CPU usage, you can keep all received metrics by removing namepass filter. This example also illustrates how to configure metrics emission to Datadog. As discussed above, you need manual configuration for outputs to Datadog regardless of configurations on Replicated.

CAUTION: This scenario leads to very large amounts of data.

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." [[outputs.datadog]]
  apikey = 'YOUR_API_KEY_HERE'
```

[discrete]
===== Scenario 3: Send limited metrics to CloudWatch

AWS charges fees for CloudWatch per series of scalar (i.e. at the level of "mean" or "sum"). Since multiple fields (e.g. mean, max, min and sum) are calculated for each metrics key (e.g. `circle.run-queue.latency`) and some fields can be redundant, you might want to select which fields are sent to CloudWatch. This can be achieved by configuring `\[[outputs.cloudwatch]]` with `fieldpass`. You also may declare `\[[outputs.cloudwatch]]` multiple times to pick up multiple metrics, as illustrated below.

```
[[inputs.statsd]]
  # Accept all metrics at input level to 1) enable output configurations without thinking of inputs, and to 2) dump discarded metrics to stdout just in case.
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "." [[outputs.cloudwatch]]
    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['circle.run-queue.latency']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['mean']

[[outputs.cloudwatch]]
    # Outputs can be specified multiple times.

    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['mem']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['available_percent']
```

== Additional Tips

You may check the logs by running `docker logs -f telegraf` to confirm your output provider (e.g. influx) is listed in the configured outputs. Additionally, if you would like to ensure that all metrics in an installation are tagged against an environment you could place the following code in your config:

```yaml
[global_tags]
Env="<staging-circleci>"
```

Please see the InfluxDB https://github.com/influxdata/influxdb#installation[documentation] for default and advanced installation steps.

CAUTION: Any changes to the config will require a restart of the CircleCI application which will require downtime.

// Extra Metics info not currently included
////
### Datadog Dashboard Configuration

This section shows you how to set up a Datadog dashboard for CircleCI metrics. We also provide descriptions of the metrics we currently support.

NOTE: CircleCI metrics are subject to change. The names of individual metrics may change, as well as their scope and monitoring options. Any changes will take place along with our usual release cycle and will be flagged up in our Changelog**

\newpage

#### The dashboard

Below is an image of our Datadog dashboard showing graphs for Make Workflow, Run queue, Time to complete Workflow, Count of Workflows completed by Status, and Build Service Latency.

![DataDog Dashboard](images/datadog-0.png)

#### JSON dashboard creation

The following JSON is for the dashboard shown above. You can use this to build the dashboard for your CircleCI Server installation:

\pagebreak

\tiny

```
{
   "notify_list":null,
   "description":"created by support@circleci.com",
   "template_variables":[

   ],
   "is_read_only":false,
   "id":"b44-4vy-w6r",
   "title":"Critical Path: Jobs",
   "url":"/dashboard/b44-4vy-w6r/critical-path-customer-builds",
   "created_at":"2018-10-25T07:28:08.108516+00:00",
   "modified_at":"2019-03-19T08:54:28.109067+00:00",
   "author_handle":"paulrobinson@circleci.com",
   "widgets":[
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.avg{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"warm",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.median{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"cool",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (mean/median) (ms)"
         },
         "id":380774989
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.95percentile{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (95th percentile - ms)"
         },
         "id":395803486
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"avg:circle.run_queue.latency.avg{platform:picard}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Run queue: Time to job started (avg) ms"
         },
         "id":381397080
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.avg{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               },
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.median{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow Mean/Median in ms (Success/Failure/Error)"
         },
         "id":395476806
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.95percentile{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow 95th percentile ms (Success/Failure/Error)"
         },
         "id":395804031
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.count{*} by {status}.as_count()",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Count of workflows completed by Status"
         },
         "id":393871870
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:builds_service.service.process_build.max{*}.rollup(max)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"avg:builds_service.service.process_build.median{*}.rollup(avg)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Build Service Latency (time to process a build)"
         },
         "id":3833057922780384
      }
   ],
   "layout_type":"ordered"
}
```

\normalsize

#### The Metrics

Following are descriptions of the specific metrics related to workflows, followed by dashboard screengrabs with those metrics highlighted:

`workflows_conductor.messaging.make_workflow.time_since_push.avg` (gauge)

* Average time from a trigger (GitHub hook) entering CircleCI and the workflow being created, shown in milliseconds.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median` (gauge): Median time to execute a workflow, shown in milliseconds.--->

<!--`workflows_conductor.execute_workflow.time_to_complete.avg` (gauge)

* Average time to execute a workflow, shown in milliseconds.

![workflows_conductor.messaging.make_workflow.time_since_push.avg (gauge) Average time to make a workflow](images/datadog-1.png)

<!---![workflows_conductor.execute_workflow.time_to_complete.median (gauge): Median time to execute a workflow, shown in milliseconds](images/datadog-2.png)--->

<!---[workflows_conductor.messaging.make_workflow.time_since_push.median (gauge): Median time to make a workflow, shown as millisecond](images/datadog-3.png)--->

<!--![workflows_conductor.execute_workflow.time_to_complete.avg (gauge): Average time to execute a workflow](images/datadog-4.png)

\pagebreak

## Monitoring Tasks

The following section describes actions to take when a threshold is exceeded for a monitored metric, for the Workflows, API-service, Nomad, or VM service.

### Workflows

#### Workflow message timing outliers

`workflows_conductor.engine_handler.messages.timing.95percentile`

**Notes/Actions**: This metric is a good indicator that work is proceeding in a timely manner. If timing threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check for exceptions from the workflows-conductor containers.

#### Number of messages received

`workflows_conductor.engine_handler.messages.timing.count`

**Notes/Actions**: This metric is a good indicator that work is flowing through the system. If message count drops to zero, complete the following steps:

1. Restart the `workflows-conductor` container
2. Check `workflows-conductor` logs. If logging isn't happening, restart
3. Check Github webhooks are being recieved to trigger jobs
4. Check for exceptions from `workflows-conductor` or `frontend` containers

#### Average time taken for Workflows to complete

`workflows_conductor.execute_workflow.time_to_complete.avg`

**Notes/Actions**: Some variation here is expected due to fluctuations in job and usage queue times. If threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check `domain-service` logs. If logging isn't happening, restart.
3. Check `contexts-service` logs. If logging isn't happening, restart.
4. Check `permissions-service` logs. If logging isn't happening, restart.
5. Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service` containers.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median`
Indicates TBD, if threshold is exceeded, complete the following steps:
1. TBD
2. TBD
3. TBD--->

<!--#### Workflows conductor memory used

`jvm.memory.total.used`

**Tag filter**: `service:workflows-conductor`

**Notes/Actions**: Indicates the amount of memory used by the Workflows Conductor service. If threshold is exceeded restart the `workflows-conductor`

\pagebreak

### API-service

The following metrics can be inspected to get diagnostic information on how the API service is running.

#### Average API response time

`backplane.ring.http_request.avg`

**Tag filter**: `service:api-service`

**Notes/ Actions**: Indicates the average response time from the API is increasing.

#### Number of API requests

`backplane.ring.http_request.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: Indicates a high number of API requests.

#### Maximum time to return an API response

`backplane.ring.http_request.max`

**Tag filter**: `service:api-service`

#### Slow API response speed

`backplane.ring.http_request.95percentile`

**Tag filter**: `service:api-service`

#### Number of active threads in the JVM

`jvm.thread.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: If this count goes above 1000, set `DOMAIN_SERVICE_REFRESH_USERS` environment variable to `false`.

#### GraphQL Resolver

`circleci.api_service.graphql.resolver.avg`

**Tag filter**: `service:api-service`

**Notes/Actions**: This metric can be split up using `type` tags to determine downstream service issues. If the threshold is exceeded across types, complete the following steps:

1. Take a thread dump of the api-service
2. Restart
3. Supply the thread dump with any tickets

If the slowdown is only for a subset of types, then inspect metrics for the corresponding service.

### Nomad

#### Average latency of builds in queue

`circle.run_queue.latency.avg`

**Notes/Actions**: Captures backup between CircleCI and Nomad. If threshold is exceeded, add additional capacity to Nomad or your VM pool.

## Monitor Settings

This section describes threshold settings for the Nomad, Domain, Workflows and VM Service to monitor common failure conditions and checks or corrective actions for each condition.

### Nomad

#### More than 10 recent jobs failed on {host}

`sum(last_10m):sum:build_agent.infra_failed{env:prod} by {host}.as_count() > 10`

**Notes/Actions**: This may indicate a bad host.

#### A number of builds are queued due to Nomad capacity

```
min(last_10m):avg:circle.run_queue.latency.avg /
{env:production,platform:picard} > 65000
```

**Notes/Actions**: Scale up the number of Nomad clients.

### Domain Service

#### Error rate increased

\footnotesize

```
avg(last_5m):default(sum:circle.domain_service.users.id.get.status{!status:200,!status:202}.as_count(), 0) /
default(sum:circle.domain_service.users.id.get.status{*}.as_count(), 0) >= 0.5
```
\normalsize

**Notes/Actions**: This might indicate problems with GitHub, check for exceptions in `domain-service` logs.

### Permissions Service

#### Error rate increased

\footnotesize

```
avg(last_5m):( default(sum:circle.permissions_service.permissions.get.status{status:500}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:502}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:503}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:504}.as_count(), 0) ) /
( default(sum:circle.permissions_service.permissions.get.status{status:200}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:202}.as_count(), 0) ) >= 0.2
```

\normalsize

**Notes/Actions**: This might indicate problems with `domain-service`, check for exceptions in `permissions-service` and `domain-service` logs.

### Workflows

#### gRPC error rate is elevated

```
avg(last_10m):sum:grpc_response.count /
{service:workflows-conductor,!status:ok}.as_count() /
sum:grpc_response.count{service:workflows-conductor}.as_count() > 0.2
```

**Notes/Actions**: Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service`.

#### No scheduled workflows have run in the last 5 minutes

```
sum(last_5m):sum:workflows_conductor.trigger.decision /
{decision:success}.as_count() < 1
```

**Notes/Actions**: Perform the following corrective actions:

1. Check `cron-service` logs. If logging isn't happening, restart.
2. Check for exceptions from `cron-service` and `workflows-conductor`.

### VM Service

#### VM service is responding with 5x errors
\footnotesize

```
sum(last_1m):sum:circle.vm_service.vms.get.status /
{status:500}.as_count() + /
sum:circle.vm_service.vms.get.status{status:503}.as_count() + /
sum:circle.vm_service.vms.get.status{status:504}.as_count() + /
sum:circle.vm_service.vms.post.status{status:500}.as_count() + /
sum:circle.vm_service.vms.post.status{status:504}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:500}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:503}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:504}.as_count() > 3
```
\normalsize

**Notes/Actions**: Check VM service metrics to identify root cause.

#### Multiple VM service provisioning errors

```
sum(last_10m):sum:build_agent.machine.created.count /
{result:error} by {resource_class_id}.as_count() > 50
```

**Notes/Actions**: This may be indicative of an issue like rate-limiting.

#### VM machine provisioning taking too long
\footnotesize

```
avg(last_5m):avg:build_agent.machine.created.avg /
{result:succeeded,resource_class_id:l1.medium, /
!docker_layer_caching:true} > 180000
```

\normalsize

**Notes/Actions**: Check VM service metrics to look for potential problems (this monitor could also be related to disk IOPS contention).-->
////
