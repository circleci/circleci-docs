= Configuration Options
:page-description: Configuration reference for Smarter Testing

This page provides a complete reference for all configuration options available in Smarter Testing.

== Test suite configuration file

All Smarter Testing configuration is defined in `.circleci/test-suites.yml` in your project root.

=== Basic structure

[source,yaml]
----
# .circleci/test-suites.yml
---
name: ci tests
discover: <command to discover test atoms>
run: <command to run tests>
analysis: <command to run tests with coverage>
outputs:
  junit: <path to junit output>
options:
  <configuration options>
----

== Commands

=== discover (required)

The `discover` command finds all test atoms for a given test suite. The command's output is split on both newlines and whitespace to extract individual test atoms.

* The `discover` command should not execute any tests
* Every line of stdout will be interpreted as a test atom
* Any tool that prints metadata in stdout will need to be suppressed

[source,yaml]
----
discover: vitest list --filesOnly
----

=== run (required)

The `run` command executes the test atoms discovered by the `discover` command using a test runner.

"Discovered" test atoms can be specified in one of two ways:

* Use the template variable `<< test.atoms >>` in the `run` command. This will be replaced with a space-separated list of test atoms to run.
* If the template variable is not found in the `run` command, the command's stdin will be a newline-separated list of test atoms.

[source,yaml]
----
run: vitest run --reporter=junit --outputFile="<< outputs.junit >>" --bail 0 << test.atoms >>
----

=== analysis (optional)

The `analysis` command executes test atoms one at a time using a test runner with code coverage enabled. Required for xref:test-impact-analysis.adoc[Test Impact Analysis].

The test atom to analyze can be specified in one of two ways:

* Use the template variable `<< test.atoms >>` in the `analysis` command. This will be replaced with the test atom to analyze.
* If the template variable is not found in the `analysis` command, the test atom will be passed on stdin.

[source,yaml]
----
analysis: |
  vitest run --coverage.enabled \
             --coverage.all=false \
             --coverage.reporter=lcov \
             --coverage.provider=v8 \
             --coverage.reportsDirectory="$(dirname << outputs.lcov >>)" \
             --silent \
             --bail 0 \
             << test.atoms >> \
             && cat "$(dirname << outputs.lcov >>)"/*.info > << outputs.lcov >>
----

=== analysis-baseline (optional)

The `analysis-baseline` command runs a minimal test that only does imports/setup (no test logic) to compute baseline coverage data. This helps exclude shared setup code from coverage analysis.

Used when test atoms cover too many files due to global imports or framework initialization.

[source,yaml]
----
analysis-baseline: |
  vitest run --coverage.enabled \
             --coverage.all=false \
             --coverage.reporter=lcov \
             --coverage.provider=v8 \
             --coverage.reportsDirectory="$(dirname << outputs.lcov >>)" \
             --bail 0 \
             "src/baseline/noop.test.ts" \
             && cat "$(dirname << outputs.lcov >>)"/*.info > << outputs.lcov >>
----

See xref:test-impact-analysis.adoc#analysis-baseline-command[Use the analysis-baseline command] for more details.

=== file-mapper (optional)

The `file-mapper` command creates a mapping of test atoms to related test files. Required when test atoms are not file names (e.g., Go packages).

The test atom to map can be specified in one of two ways:

* Use the template variable `<< test.atoms >>` in the `file-mapper` command.
* If the template variable is not found, the test atom will be passed on stdin.

[source,yaml]
----
file-mapper: go list -json="Dir,ImportPath,TestGoFiles,XTestGoFiles" ./... > << outputs.go-list-json >>
----

See xref:test-impact-analysis.adoc#file-mapper[Use the file-mapper command] for more details.

=== runner (optional)

The `runner` field uses a built-in configuration for common test runners. Available runners: `vitest`, `jest`, `yarn-jest`, `pytest`, `go`, `gotestsum`.

[source,yaml]
----
runner: vitest
----

You can override built-in commands if necessary:

[source,yaml]
----
runner: go
discover: find . -type f -name '*_test.go' -exec dirname {} \; | sort -u
----

== Template variables

Template variables can be used in commands to reference dynamic values:

=== Test atom variables

* `<< test.atoms >>` - Space-separated list of test atoms (in `run` command) or single test atom (in `analysis` command)

=== Output variables

* `<< outputs.junit >>` - Path to JUnit XML output file
* `<< outputs.lcov >>` - Path to LCOV coverage output file
* `<< outputs.go-coverage >>` - Path to Go coverage output file
* `<< outputs.go-list-json >>` - Path to Go list JSON output file

== Outputs

The `outputs` section defines where test results and coverage data are written.

[source,yaml]
----
outputs:
  junit: test-reports/tests.xml
  lcov: coverage/lcov.info
  go-coverage: coverage.out
  go-list-json: go-list.json
----

The `junit` output is required. Coverage outputs are required when using Test Impact Analysis.

== Options

The `options` section configures Smarter Testing features and behavior.

[.table-scroll]
--
[cols=3*, options="header"]
|===
|Option|Default|Description

|`dynamic-test-splitting`
|`true`
|Whether the tests should be distributed across a shared queue and fetched across multiple dynamic batches. If a test runner has slow start up time per batch, disabling this can speed up tests.

|`test-impact-analysis`
|`false`
|Enables Test Impact Analysis features, such as intelligent test selection based on code changes.

|`timeout`
|`10`
|The time in minutes a step will wait for tests to become available when running in parallel.

|`full-test-run-paths`
a|
* `.circleci/*.yml`
* `go.mod`
* `go.sum`
* `package-lock.json`
* `package.json`
* `project.clj`
* `yarn.lock`
|A list of paths that might have an indirect impact on tests and should run the full test suite if a change is detected. To disable this option, provide an empty array: `full-test-run-paths: []`

|`test-analysis-duration`
|`null`
|The maximum duration test analysis will run for in minutes. Any remaining tests will be analysed the next time test analysis is run. Useful for timeboxing analysis on main branches.

|`impact-key`
|`"default"`
|Group relevant impact data together using a matching key within the same project. Use when you have multiple test suites in a single repository.
|===
--

=== Example configurations

**Customize full-test-run-paths:**
[source,yaml]
----
options:
  full-test-run-paths:
    - .circleci/*.yml
    - requirements.txt
    - Dockerfile
    - custom-config.json
----

**Disable dynamic test splitting:**
[source,yaml]
----
options:
  dynamic-test-splitting: false
----

**Enable Test Impact Analysis with timeboxed analysis:**
[source,yaml]
----
options:
  test-impact-analysis: true
  test-analysis-duration: 10
----

**Multiple test suites with separate impact keys:**
[source,yaml]
----
---
name: service-1 tests
options:
  test-impact-analysis: true
  impact-key: service-1
---
name: service-2 tests
options:
  test-impact-analysis: true
  impact-key: service-2
----

[#cli-flags]
== CLI flags

Flags can be passed to the `circleci run testsuite` command to control behavior.

[.table-scroll]
--
[cols=3*, options="header"]
|===
|Flag|Default|Description

|`--verbose`
|`false`
|Provides additional details in output.

|`--test-analysis=all\|impacted\|none`
|On branch `main`: `impacted` +
On other branches: `none` +
When run locally: `none`
a|
* `all` - Analyzes all discovered tests, used to override any existing impact data
* `impacted` - Analyzes only tests impacted by a change, used to refresh impact data
* `none` - Skips analysis

|`--test-selection=all\|impacted\|none`
|On branch `main`: `all` +
On other branches: `impacted` +
When run locally: `impacted`
a|
* `all` - Selects and runs all discovered tests, used to run the full test suite
* `impacted` - Selects and runs only the tests impacted by a change
* `none` - Skips running tests, used to skip straight to analysis

|`--local`
|`false`
|Run in local mode. Uses local impact data and disables API calls to CircleCI.

|`--project-id`
|Auto-detected
|CircleCI project ID. Required when project cannot be auto-detected.

|`--circleci-token`
|From `circleci setup`
|CircleCI API token. Required when running without `--local` and token not configured via CLI.
|===
--

=== Example usage

**Run analysis on a feature branch:**
[source,console]
----
$ circleci run testsuite "ci tests" --test-selection=none --test-analysis=impacted
----

**Run all tests without selection:**
[source,console]
----
$ circleci run testsuite "ci tests" --test-selection=all
----

**Run locally with impact data from CircleCI:**
[source,console]
----
$ circleci run testsuite "ci tests"
----

**Run locally with local impact data only:**
[source,console]
----
$ circleci run testsuite "ci tests" --local
----

**Verbose output for debugging:**
[source,console]
----
$ circleci run testsuite "ci tests" --verbose
----

== CircleCI configuration

=== Using conditional expressions

You can use CircleCI pipeline values and expressions in your configuration to control behavior based on branch, parameters, or other conditions.

**Run analysis on specific branch:**
[source,yaml]
----
# .circleci/config.yml
version: 2.1
jobs:
  test:
    executor: node-with-service
    steps:
      - setup
      - run: |
          circleci run testsuite "ci tests" \
            --test-analysis=<< pipeline.git.branch == "develop" and "impacted" or "none" >>
      - store_test_results:
          path: test-reports
----

**Conditional parallelism:**
[source,yaml]
----
# .circleci/config.yml
version: 2.1
jobs:
  test:
    executor: node-with-service
    parallelism: << pipeline.git.branch == "main" and 10 or 4 >>
    steps:
      - setup
      - run: circleci run testsuite "ci tests"
      - store_test_results:
          path: test-reports
----

**Scheduled analysis pipeline:**
[source,yaml]
----
# .circleci/config.yml
version: 2.1
parameters:
  run-scheduled-analysis:
    type: boolean
    default: false
jobs:
  analysis:
    executor: node-with-service
    steps:
      - setup
      - run: circleci run testsuite "scheduled tests"
  test:
    executor: node-with-service
    steps:
      - setup
      - run: circleci run testsuite "main tests"
      - store_test_results:
          path: test-reports
workflows:
  scheduled-analysis:
    when: pipeline.parameters.run-scheduled-analysis
    jobs:
      - analysis
  main:
    when: not pipeline.parameters.run-scheduled-analysis
    jobs:
      - test
----

=== Multiple test suites

You can define multiple test suites in a single `.circleci/test-suites.yml` file:

[source,yaml]
----
# .circleci/test-suites.yml
---
name: unit tests
discover: vitest list --filesOnly src/**/*.test.ts
run: vitest run --reporter=junit --outputFile="<< outputs.junit >>" << test.atoms >>
outputs:
  junit: test-reports/unit-tests.xml
---
name: integration tests
discover: vitest list --filesOnly tests/**/*.test.ts
run: vitest run --reporter=junit --outputFile="<< outputs.junit >>" << test.atoms >>
outputs:
  junit: test-reports/integration-tests.xml
----

Then run each suite separately in your CircleCI configuration:

[source,yaml]
----
# .circleci/config.yml
version: 2.1
jobs:
  unit-tests:
    executor: node-with-service
    steps:
      - setup
      - run: circleci run testsuite "unit tests"
      - store_test_results:
          path: test-reports
  integration-tests:
    executor: node-with-service
    steps:
      - setup
      - run: circleci run testsuite "integration tests"
      - store_test_results:
          path: test-reports
workflows:
  test:
    jobs:
      - unit-tests
      - integration-tests
----

== Complete example

Here's a complete example showing all major configuration options:

[source,yaml]
----
# .circleci/test-suites.yml
---
name: ci tests
discover: vitest list --filesOnly
run: vitest run --reporter=junit --outputFile="<< outputs.junit >>" --bail 0 << test.atoms >>
analysis: |
  vitest run --coverage.enabled \
             --coverage.all=false \
             --coverage.reporter=lcov \
             --coverage.provider=v8 \
             --coverage.reportsDirectory="$(dirname << outputs.lcov >>)" \
             --silent \
             --bail 0 \
             << test.atoms >> \
             && cat "$(dirname << outputs.lcov >>)"/*.info > << outputs.lcov >>
analysis-baseline: |
  vitest run --coverage.enabled \
             --coverage.all=false \
             --coverage.reporter=lcov \
             --coverage.provider=v8 \
             --coverage.reportsDirectory="$(dirname << outputs.lcov >>)" \
             --bail 0 \
             "src/baseline/noop.test.ts" \
             && cat "$(dirname << outputs.lcov >>)"/*.info > << outputs.lcov >>
outputs:
  junit: test-reports/tests.xml
options:
  test-impact-analysis: true
  test-analysis-duration: 10
  dynamic-test-splitting: true
  timeout: 15
  full-test-run-paths:
    - .circleci/*.yml
    - package.json
    - package-lock.json
  impact-key: main-app
----

[source,yaml]
----
# .circleci/config.yml
version: 2.1
jobs:
  test:
    executor: node-with-service
    parallelism: 4
    steps:
      - checkout
      - restore_cache:
          keys:
            - deps-{{ checksum "package-lock.json" }}
      - run: npm ci
      - run: circleci run testsuite "ci tests"
      - store_test_results:
          path: test-reports
workflows:
  test:
    jobs:
      - test
----
