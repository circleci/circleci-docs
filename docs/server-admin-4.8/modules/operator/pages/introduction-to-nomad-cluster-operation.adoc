= Introduction to Nomad cluster operation
:page-noindex: true
:page-platform: Server v4.8, Server Admin
:page-description: Learn how to operate the Nomad Cluster in your CircleCI Server v4.8 installation.
:experimental:

CircleCI uses link:https://www.nomadproject.io/[Nomad] as the primary job scheduler. This section provides a basic introduction to Nomad for understanding how to operate the Nomad Cluster in your CircleCI installation.

[#basic-terminology-and-architecture]
== Basic terminology and architecture

.Nomad Cluster Management
image::guides:ROOT:nomad-diagram-v2.png[Diagram of the Nomad cluster]
<<<
- **Nomad server:** Nomad servers are the brains of the cluster. They receive and allocate jobs to Nomad clients. In CircleCI Server, a Nomad server runs as a service in your Kubernetes cluster.

- **Nomad client:** Nomad clients execute the jobs they are allocated by Nomad servers. Usually a Nomad client runs on a dedicated machine (often a VM) to take full advantage of machine power. You can have multiple Nomad clients to form a cluster and the Nomad server allocates jobs to the cluster with its scheduling algorithm.

- **Nomad jobs:** A Nomad job is a specification, provided by a user, that declares a workload for Nomad. A Nomad job corresponds to an execution of a CircleCI job. If the job uses parallelism, for example `parallelism: 10`, then Nomad runs 10 jobs. For more information on parallelism, see the xref:guides:optimize:parallelism-faster-jobs.adoc#[Test Splitting and Parallelism] guide.

- **Build agent:** Build agent is a Go program written by CircleCI that executes steps in a job and reports the results. Build agent is executed as the main process inside a Nomad job.

[#basic-operations]
== Basic operations

The following section is a basic guide to operating a Nomad cluster in your installation.

The `nomad` CLI is installed in the Nomad pod. It is preconfigured to talk to the Nomad cluster, so it is possible to use `kubectl` along with the `nomad` command to run the commands in this section.

[#checking-the-jobs-status]
=== Checking the jobs status

The get a list of statuses for all jobs in your cluster, run the following command:

[source,console]
----
$ kubectl exec -it <nomad-server-pod-ID> -- nomad status
----

The `Status` is the most important field in the output, with the following status type definitions:

- `running`: Nomad has started executing the job. This typically means your job in CircleCI is started.

- `pending`: There are not enough resources available to execute the job inside the cluster.

- `dead`: Nomad has finished executing the job. The status becomes `dead` regardless of whether the corresponding CircleCI job/build succeeds or fails.

[#checking-the-cluster-status]
=== Checking the cluster status

To get a list of your Nomad clients, run the following command:

[source,console]
----
$ kubectl exec -it <nomad-server-pod-ID> -- nomad node-status
----

NOTE: `nomad node-status` reports both Nomad clients that are currently serving (status `active`) and Nomad clients that were taken out of the cluster (status `down`). Therefore, you need to count the number of `active` Nomad clients to know the current capacity of your cluster.

To get more information about a specific client, run the following command from that client:

[source,console]
----
$ kubectl exec -it <nomad-server-pod-ID> -- nomad node-status -self
----

This gives information such as how many jobs are running on the client and the resource utilization of the client.

[#checking-logs]
=== Checking logs

A Nomad job corresponds to an execution of a CircleCI job. Therefore, Nomad job logs can sometimes help to understand the status of a CircleCI job if there is a problem. To get logs for a specific job, run the following command:

[source,console]
----
$ kubectl exec -it <nomad-server-pod-ID> -- nomad logs -job -stderr <nomad-job-id>
----

NOTE: Be sure to specify the `-stderr` flag, as this is where most Build Agent logs appear.

While the `nomad logs -job` command is useful, it is not always accurate because the `-job` flag uses a random allocation of the specified job. The term `allocation` is a smaller unit in Nomad Job, which is beyond the scope of this document. To learn more, see link:https://www.nomadproject.io/docs/internals/scheduling.html[the official document].

Complete the following steps to get logs from the allocation of the specified job:

. Get the job ID with `nomad status` command.
. Get the allocation ID of the job with `nomad status <job-id>` command.
. Get the logs from the allocation with `nomad logs -stderr <allocation-id>`.

[#accessing-the-nomad-web-ui]
=== Accessing the Nomad Web UI

Nomad provides a web UI for inspecting your Nomad cluster. If you are using an internalized Nomad deployment, which is the default setup with CircleCI Server, follow the instructions in this section to temporarily access the UI for troubleshooting purposes. For a more permanent solution, consider externalizing Nomad and consult the official Nomad documentation for setting up routing.

. When using Nomad in Kubernetes, it binds to its Pod IP address. Use the command below to fetch the IP address of the Nomad server:
+
[source,console]
----
$ export NOMAD_ADDR=$(kubectl get svc nomad-server -o jsonpath='{.spec.clusterIP}' -n <server-namespace>)
----

. If using Externalized Nomad, Use Nomad Network Load Balancer or DNS address to port-forward to access the web UI.

. To port-forward to the Nomad service, set up a lightweight tunnelling mechanism within the cluster as follows:
+
[source,console]
----
$ kubectl run nomad-tunnel --rm -it --restart=Never --image=alpine/socat -n <server-namespace> -- TCP-LISTEN:4646,fork,reuseaddr TCP:$NOMAD_ADDR:4646
----

. In another terminal, run the following command to set up port forwarding:
+
[source,console]
----
$ kubectl port-forward pod/nomad-tunnel 4646:4646 -n <server-namespace>
----

. Navigate to `++http[s]://localhost:4646/ui++` in your browser to access the Nomad UI. For more information on utilizing the Nomad UI, refer to the link:https://developer.hashicorp.com/nomad/tutorials/web-ui[Nomad documentation].

[#shutting-down-a-nomad-client]
=== Shutting down a Nomad client

When you want to shut down a Nomad client, you must first set the client to `drain` mode. In `drain` mode, the client will finish any jobs that have already been allocated but will not be allocated any new jobs.

. To drain a client, log in to the client and set the client to drain mode with `node-drain` command as follows:
+
[source,console]
----
$ nomad node-drain -self -enable
----
. Then, make sure the client is in drain mode using the `node-status` command:
+
[source,console]
----
$ nomad node-status -self
----

Alternatively, you can drain a remote node with the following command, substituting the node ID:

[source,console]
----
$ nomad node-drain -enable -yes <node-id>
----

[#scaling-down-the-client-cluster]
=== Scaling down the client cluster

To set up a mechanism for clients to shutdown, first enter `drain` mode, then wait for all jobs to be finished before terminating the client. You can also configure an link:https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html[ASG Lifecycle Hook] that triggers a script for scaling down instances.

The script should use the commands in the section above to do the following:

. Put the instance in drain mode.
. Monitor running jobs on the instance and wait for them to finish.
. Terminate the instance.

[#externalize-servers]
== Externalize your Nomad servers
From server v4.8, Nomad Servers may now be deployed externally to your Kubernetes cluster that hosts your installation of CircleCI Server. Externalization of Nomad Servers is optional. Externalization of Nomad Servers can improve their stability. If you already have a CircleCI Server instance with _internal_ Nomad Servers, the process to switch to external Nomad Servers is as follows:

. Stop all builds on your CircleCI Server instance.
. Follow our installation instructions for deploying Nomad Servers on either xref:installation:phase-3-aws-execution-environments.adoc#nomad-servers[AWS] or xref:installation:phase-3-gcp-execution-environments.adoc#nomad-servers[GCP].

=== Why externalize your Nomad servers?
Externalizing your Nomad servers means running them on dedicated virtual machine (VM) instances outside of Kubernetes, rather than as pods within your K8s cluster. When both Nomad servers and clients reside in cloud VM instances, you can improve the stability and operational simplicity of your Nomad cluster. Here are the key benefits of externalizing Nomad servers:

**Stability and reduced complexity**:: Running your Nomad server on a VM provides a stable foundation. When a Nomad server runs in a Kubernetes pod, it creates a circular dependency where Kubernetes manages your scheduler, which could be managing Kubernetes itself. This adds operational complexity and potential failure modes.

**Resource predictability**:: VM instances provide dedicated, predictable resources for Nomad servers. In Kubernetes, pods can be evicted, rescheduled, or affected by noisy neighbors. For the control plane of an orchestration system, we want maximum stability and resource guarantees, which VM instances naturally provide.

**Direct communication**:: Nomad servers and clients communicate directly over virtual private cloud (VPC) networking without crossing orchestration boundaries. This eliminates the need to manage Kubernetes services, ingress controllers, or service meshes for Nomad's internal communication (RPC, Gossip, Raft). This makes debugging, security group configuration, and network troubleshooting much simpler, especially when diagnosing cluster consensus or client connectivity issues.

**Clearer upgrade and rollback strategy**:: With Nomad servers/clients running in VMs, you have clear separation between the Kubernetes orchestration layer and the Nomad control plane. You can upgrade Nomad independently of Kubernetes, avoiding version compatibility concerns between the two systems. VM-based deployments also provide more straightforward rollback mechanisms using AMIs, snapshots, or infrastructure-as-code versioning, compared to managing StatefulSets with persistent volumes in Kubernetes.

**Consistent deployment model**:: You can use the same deployment patterns, monitoring, logging, and configuration management across your entire Nomad cluster which provides better monitoring and reduces operational complexity.

ifndef::pdf[]
[#next-steps]
== Next steps

* Read the xref:managing-user-accounts.adoc#[Managing User Accounts] guide.
endif::[]
